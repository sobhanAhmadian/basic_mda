{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbSkLzXRVkG1","executionInfo":{"status":"ok","timestamp":1707669906680,"user_tz":-210,"elapsed":47382,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"0c5bb889-1ff6-4b44-fa19-ecbf14c8c36f"},"id":"AbSkLzXRVkG1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"initial_id","metadata":{"collapsed":true,"id":"initial_id","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707669906680,"user_tz":-210,"elapsed":10,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"bc47d254-b4f4-48f8-890d-f4582d9f3501"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Academic/Topics/AI/Machine\\ Learning\\ Dr.\\ Montazeri/Project/ml_mda"],"metadata":{"id":"uDVTuQuDVXDp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707669907698,"user_tz":-210,"elapsed":1021,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"2dc91fee-c41c-4e11-ce1b-3adedeb3a354"},"id":"uDVTuQuDVXDp","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Academic/Topics/AI/Machine Learning Dr. Montazeri/Project/ml_mda\n"]}]},{"cell_type":"code","source":["!pip install torch_geometric"],"metadata":{"id":"RDY-Ibk-EuwN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707669916633,"user_tz":-210,"elapsed":8938,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"0a867784-a367-4ace-a04d-8c219fbc8f97"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}],"id":"RDY-Ibk-EuwN"},{"cell_type":"code","source":["!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html"],"metadata":{"id":"8Vot3XZDEzNX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707669930447,"user_tz":-210,"elapsed":13820,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"bff8d576-74ba-4487-c734-309a45a0c4ec"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/pyg_lib-0.4.0%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (932 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.1/932.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.4.0+pt21cu121 torch_cluster-1.6.3+pt21cu121 torch_scatter-2.1.2+pt21cu121 torch_sparse-0.6.18+pt21cu121 torch_spline_conv-1.2.2+pt21cu121\n"]}],"id":"8Vot3XZDEzNX"},{"cell_type":"markdown","source":["# Requirements"],"metadata":{"id":"CP5slJxnWMG-"},"id":"CP5slJxnWMG-"},{"cell_type":"code","source":["import torch\n","\n","from torch.optim import Adam\n","from torch_geometric.nn import ComplEx, DistMult, RotatE, TransE\n","from torch_geometric.data import Data\n","\n","from base import OptimizerConfig, cross_validation\n","from base import SimplePytorchData, SimplePytorchDataTrainTestSplit\n","from base import SimpleTrainer, SimpleTester\n","from src.config import SimpleClassifierConfig, GraphAutoEncoderConfig, KGEConfig\n","from src.features import get_relations, get_entities, get_associations, get_homogeneous_graph, get_kge_pair_embedd_for_training_data\n","from src.models import SimpleMDAClassifier, SimpleMDAClassifierFactory\n","from src.utils import train_test_sampler, prj_logger\n","from torch_geometric.nn import GCNConv"],"metadata":{"id":"1Tz_6Gnq191K","executionInfo":{"status":"ok","timestamp":1707669951538,"user_tz":-210,"elapsed":21093,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"1Tz_6Gnq191K","execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"5V-nDmj61M_3","executionInfo":{"status":"ok","timestamp":1707669951539,"user_tz":-210,"elapsed":17,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"5V-nDmj61M_3","execution_count":7,"outputs":[]},{"cell_type":"code","source":["import logging\n","import sys\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    handlers=[\n","        logging.StreamHandler(stream=sys.stdout)\n","    ],\n","    force=True\n",")"],"metadata":{"id":"v4fhFqHr-UQI","executionInfo":{"status":"ok","timestamp":1707669951539,"user_tz":-210,"elapsed":16,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"v4fhFqHr-UQI","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# ComplEx"],"metadata":{"id":"jQ9m4sXXMrP_"},"id":"jQ9m4sXXMrP_"},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"mze45lXFT9mw"},"id":"mze45lXFT9mw"},{"cell_type":"code","source":["kge_optimizer_config = OptimizerConfig()\n","kge_optimizer_config.optimizer = torch.optim.Adam\n","kge_optimizer_config.lr = 0.01\n","kge_optimizer_config.batch_size = 1000\n","kge_optimizer_config.n_epoch = 30\n","kge_optimizer_config.exp_name = \"Optimizer for Graph Auto Encoder\"\n","kge_optimizer_config.device = device"],"metadata":{"id":"r4zjaDCMMsJC","executionInfo":{"status":"ok","timestamp":1707669951539,"user_tz":-210,"elapsed":16,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"r4zjaDCMMsJC","execution_count":9,"outputs":[]},{"cell_type":"code","source":["kge_model_config = KGEConfig()\n","kge_model_config.kge = ComplEx\n","kge_model_config.hidden_channels = 32"],"metadata":{"id":"KEPyGw8LNBFv","executionInfo":{"status":"ok","timestamp":1707669951540,"user_tz":-210,"elapsed":17,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"KEPyGw8LNBFv","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Embedding"],"metadata":{"id":"LKtvRPGwT_NG"},"id":"LKtvRPGwT_NG"},{"cell_type":"code","source":["md_embed = get_kge_pair_embedd_for_training_data(kge_model_config, kge_optimizer_config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2Qz4EGMOrrk","executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":984260,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"cf463e15-d9e1-474a-8b5c-491e68a9b660"},"id":"r2Qz4EGMOrrk","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 16:45:51,550 [INFO] Calling get_node2vec_pair_embedd on cpu device ...\n","2024-02-11 16:45:51,552 [INFO] Calling get_homogeneous_graph\n","2024-02-11 16:45:54,582 [INFO] homogeneous data : Data(x=[66911, 1], edge_index=[2, 633662])\n","2024-02-11 16:45:54,591 [INFO] Calling get_kge_embedd on cpu device ...\n","2024-02-11 16:45:54,593 [INFO] Calling get_knowledge_graph\n","2024-02-11 16:45:55,686 [INFO] knowledge graph data : Data(num_nodes=66911, edge_index=[2, 633662], edge_type=[633662])\n","2024-02-11 16:45:55,707 [INFO] Setting num relations and num nodes for kge config to 39 and 66911\n","2024-02-11 16:45:55,710 [INFO] Creating KGE model ...\n","2024-02-11 16:45:55,712 [INFO] Initialing MDATransE with model_config {'model_name': None}\n","2024-02-11 16:45:55,785 [INFO] Training KGE ...\n","2024-02-11 16:45:55,786 [INFO] Running KGETrainer with Optimizer for Graph Auto Encoder\n","2024-02-11 16:45:55,789 [INFO] Creating <class 'torch.optim.adam.Adam'> with lr : 0.01\n","2024-02-11 16:45:55,792 [INFO] moving model to cpu\n","2024-02-11 16:46:34,060 [INFO] Epoch: 001, Loss: 0.4257\n","2024-02-11 16:47:06,347 [INFO] Epoch: 002, Loss: 0.1689\n","2024-02-11 16:47:40,420 [INFO] Epoch: 003, Loss: 0.1167\n","2024-02-11 16:48:15,201 [INFO] Epoch: 004, Loss: 0.0979\n","2024-02-11 16:48:49,871 [INFO] Epoch: 005, Loss: 0.0887\n","2024-02-11 16:49:24,180 [INFO] Epoch: 006, Loss: 0.0819\n","2024-02-11 16:49:58,744 [INFO] Epoch: 007, Loss: 0.0773\n","2024-02-11 16:50:33,882 [INFO] Epoch: 008, Loss: 0.0739\n","2024-02-11 16:51:04,624 [INFO] Epoch: 009, Loss: 0.0695\n","2024-02-11 16:51:39,057 [INFO] Epoch: 010, Loss: 0.0670\n","2024-02-11 16:52:13,437 [INFO] Epoch: 011, Loss: 0.0653\n","2024-02-11 16:52:44,658 [INFO] Epoch: 012, Loss: 0.0625\n","2024-02-11 16:53:18,741 [INFO] Epoch: 013, Loss: 0.0612\n","2024-02-11 16:53:47,999 [INFO] Epoch: 014, Loss: 0.0593\n","2024-02-11 16:54:19,381 [INFO] Epoch: 015, Loss: 0.0582\n","2024-02-11 16:54:48,581 [INFO] Epoch: 016, Loss: 0.0569\n","2024-02-11 16:55:23,309 [INFO] Epoch: 017, Loss: 0.0557\n","2024-02-11 16:55:53,141 [INFO] Epoch: 018, Loss: 0.0542\n","2024-02-11 16:56:21,713 [INFO] Epoch: 019, Loss: 0.0540\n","2024-02-11 16:56:54,715 [INFO] Epoch: 020, Loss: 0.0529\n","2024-02-11 16:57:26,284 [INFO] Epoch: 021, Loss: 0.0524\n","2024-02-11 16:57:57,398 [INFO] Epoch: 022, Loss: 0.0519\n","2024-02-11 16:58:32,432 [INFO] Epoch: 023, Loss: 0.0510\n","2024-02-11 16:59:06,239 [INFO] Epoch: 024, Loss: 0.0507\n","2024-02-11 16:59:33,856 [INFO] Epoch: 025, Loss: 0.0505\n","2024-02-11 17:00:08,577 [INFO] Epoch: 026, Loss: 0.0504\n","2024-02-11 17:00:42,557 [INFO] Epoch: 027, Loss: 0.0499\n","2024-02-11 17:01:10,298 [INFO] Epoch: 028, Loss: 0.0491\n","2024-02-11 17:01:41,362 [INFO] Epoch: 029, Loss: 0.0492\n","2024-02-11 17:02:15,202 [INFO] Epoch: 030, Loss: 0.0488\n","2024-02-11 17:02:15,204 [INFO] Result on Train Data : {'AUC': 0, 'ACC': 0, 'F1 Score': 0, 'AUPR': 0, 'Loss': 0.04881423553139339}\n","2024-02-11 17:02:15,205 [INFO] loss of KGE model : 0.04881423553139339\n","2024-02-11 17:02:15,227 [INFO] node embedding shape : torch.Size([66911, 32])\n","2024-02-11 17:02:15,230 [INFO] disease embedding shape : torch.Size([898, 32])\n","2024-02-11 17:02:15,233 [INFO] microbe embedding shape : torch.Size([898, 32])\n","2024-02-11 17:02:15,235 [INFO] microbe disease combination embedding shape : torch.Size([898, 64])\n"]}]},{"cell_type":"markdown","source":["# Classification"],"metadata":{"id":"T_hIMihJMts8"},"id":"T_hIMihJMts8"},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"ocxVXIz1MqLJ"},"id":"ocxVXIz1MqLJ"},{"cell_type":"code","source":["associations = get_associations()\n","y = torch.tensor(associations['increased'].tolist(), dtype=torch.float32).reshape(-1, 1).to(device)"],"metadata":{"id":"jEfB8KA7gPx2","executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":10,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":12,"outputs":[],"id":"jEfB8KA7gPx2"},{"cell_type":"code","source":["# Train Test Split\n","train_indices, test_indices = train_test_sampler(y.shape[0], 0.7)\n","\n","data = SimplePytorchData(md_embed, y)\n","train_data = SimplePytorchData(md_embed[train_indices], y[train_indices])\n","test_data = SimplePytorchData(md_embed[test_indices], y[test_indices])"],"metadata":{"id":"DNdQlgzMMtHN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":9,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"6f767f13-2322-4f87-95e5-1503f3aced2a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 17:02:15,267 [INFO] Initializing SimplePytorchData with X shape : torch.Size([898, 64]) and y shape : torch.Size([898, 1])\n","2024-02-11 17:02:15,271 [INFO] Initializing SimplePytorchData with X shape : torch.Size([628, 64]) and y shape : torch.Size([628, 1])\n","2024-02-11 17:02:15,273 [INFO] Initializing SimplePytorchData with X shape : torch.Size([270, 64]) and y shape : torch.Size([270, 1])\n"]}],"id":"DNdQlgzMMtHN"},{"cell_type":"markdown","source":["## Classifier"],"metadata":{"id":"ye_6wl6nxmhs"},"id":"ye_6wl6nxmhs"},{"cell_type":"code","source":["simple_classifier_config = SimpleClassifierConfig()\n","simple_classifier_config.model_name = \"simple classifier\"\n","simple_classifier_config.input_dim = md_embed.shape[1]\n","simple_classifier_config.hidden_dim = 32\n","simple_classifier_config.output_dim = 1\n","simple_classifier_config.num_layers = 2\n","simple_classifier_config.dropout = 0.1"],"metadata":{"id":"BFTQsCl8M9bv","executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":6,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":14,"outputs":[],"id":"BFTQsCl8M9bv"},{"cell_type":"code","source":["mda_classifier = SimpleMDAClassifier(simple_classifier_config)"],"metadata":{"id":"1ciyBQ4QM_0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":6,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"c3287f00-a4a5-428a-935a-29d63312aea8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 17:02:15,293 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:15,295 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n"]}],"id":"1ciyBQ4QM_0U"},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{"id":"s_5cdKvOx4q5"},"id":"s_5cdKvOx4q5"},{"cell_type":"code","source":["classifier_optimizer_config = OptimizerConfig()\n","classifier_optimizer_config.optimizer = torch.optim.Adam\n","classifier_optimizer_config.criterion = torch.nn.BCEWithLogitsLoss()\n","classifier_optimizer_config.lr = 0.01\n","classifier_optimizer_config.batch_size = 32\n","classifier_optimizer_config.n_epoch = 50\n","classifier_optimizer_config.exp_name = \"adam optimizer\"\n","classifier_optimizer_config.save = False\n","classifier_optimizer_config.save_path = None\n","classifier_optimizer_config.device = device\n","classifier_optimizer_config.report_size = 10  # batch to report ratio\n","classifier_optimizer_config.threshold = 0.5"],"metadata":{"id":"3D6yhiPpNEc8","executionInfo":{"status":"ok","timestamp":1707670935783,"user_tz":-210,"elapsed":5,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":16,"outputs":[],"id":"3D6yhiPpNEc8"},{"cell_type":"markdown","source":["## Train Test Approach"],"metadata":{"id":"4iI5bMmJNQV3"},"id":"4iI5bMmJNQV3"},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"h24KnmDZNAgD"},"id":"h24KnmDZNAgD"},{"cell_type":"code","source":["train_result = SimpleTrainer().train(model=mda_classifier,\n","                                     data=train_data,\n","                                     config=classifier_optimizer_config)"],"metadata":{"id":"OqKrF7HmNFx1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670938309,"user_tz":-210,"elapsed":2531,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"d5fb6d07-c344-4754-e684-cf5fcf7cc096"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 17:02:15,318 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:15,321 [INFO] moving data and model to cpu\n","2024-02-11 17:02:15,364 [INFO] loss: 0.0192    [1,    10]\n","2024-02-11 17:02:15,381 [INFO] loss: 0.0161    [1,    20]\n","2024-02-11 17:02:15,398 [INFO] loss: 0.0142    [2,    10]\n","2024-02-11 17:02:15,423 [INFO] loss: 0.0128    [2,    20]\n","2024-02-11 17:02:15,442 [INFO] loss: 0.0136    [3,    10]\n","2024-02-11 17:02:15,458 [INFO] loss: 0.0097    [3,    20]\n","2024-02-11 17:02:15,476 [INFO] loss: 0.0105    [4,    10]\n","2024-02-11 17:02:15,492 [INFO] loss: 0.0112    [4,    20]\n","2024-02-11 17:02:15,510 [INFO] loss: 0.0093    [5,    10]\n","2024-02-11 17:02:15,527 [INFO] loss: 0.0104    [5,    20]\n","2024-02-11 17:02:15,543 [INFO] loss: 0.0099    [6,    10]\n","2024-02-11 17:02:15,560 [INFO] loss: 0.0085    [6,    20]\n","2024-02-11 17:02:15,576 [INFO] loss: 0.0083    [7,    10]\n","2024-02-11 17:02:15,592 [INFO] loss: 0.0094    [7,    20]\n","2024-02-11 17:02:15,609 [INFO] loss: 0.0080    [8,    10]\n","2024-02-11 17:02:15,625 [INFO] loss: 0.0082    [8,    20]\n","2024-02-11 17:02:15,641 [INFO] loss: 0.0077    [9,    10]\n","2024-02-11 17:02:15,657 [INFO] loss: 0.0079    [9,    20]\n","2024-02-11 17:02:15,676 [INFO] loss: 0.0074    [10,    10]\n","2024-02-11 17:02:15,693 [INFO] loss: 0.0068    [10,    20]\n","2024-02-11 17:02:15,709 [INFO] loss: 0.0058    [11,    10]\n","2024-02-11 17:02:15,726 [INFO] loss: 0.0079    [11,    20]\n","2024-02-11 17:02:15,744 [INFO] loss: 0.0066    [12,    10]\n","2024-02-11 17:02:15,760 [INFO] loss: 0.0061    [12,    20]\n","2024-02-11 17:02:15,777 [INFO] loss: 0.0050    [13,    10]\n","2024-02-11 17:02:15,793 [INFO] loss: 0.0066    [13,    20]\n","2024-02-11 17:02:15,810 [INFO] loss: 0.0056    [14,    10]\n","2024-02-11 17:02:15,827 [INFO] loss: 0.0063    [14,    20]\n","2024-02-11 17:02:15,844 [INFO] loss: 0.0062    [15,    10]\n","2024-02-11 17:02:15,865 [INFO] loss: 0.0052    [15,    20]\n","2024-02-11 17:02:15,883 [INFO] loss: 0.0050    [16,    10]\n","2024-02-11 17:02:15,900 [INFO] loss: 0.0045    [16,    20]\n","2024-02-11 17:02:15,917 [INFO] loss: 0.0049    [17,    10]\n","2024-02-11 17:02:15,933 [INFO] loss: 0.0046    [17,    20]\n","2024-02-11 17:02:15,956 [INFO] loss: 0.0050    [18,    10]\n","2024-02-11 17:02:15,972 [INFO] loss: 0.0041    [18,    20]\n","2024-02-11 17:02:15,991 [INFO] loss: 0.0035    [19,    10]\n","2024-02-11 17:02:16,011 [INFO] loss: 0.0052    [19,    20]\n","2024-02-11 17:02:16,029 [INFO] loss: 0.0047    [20,    10]\n","2024-02-11 17:02:16,045 [INFO] loss: 0.0049    [20,    20]\n","2024-02-11 17:02:16,066 [INFO] loss: 0.0042    [21,    10]\n","2024-02-11 17:02:16,092 [INFO] loss: 0.0044    [21,    20]\n","2024-02-11 17:02:16,116 [INFO] loss: 0.0047    [22,    10]\n","2024-02-11 17:02:16,143 [INFO] loss: 0.0033    [22,    20]\n","2024-02-11 17:02:16,165 [INFO] loss: 0.0023    [23,    10]\n","2024-02-11 17:02:16,186 [INFO] loss: 0.0054    [23,    20]\n","2024-02-11 17:02:16,208 [INFO] loss: 0.0034    [24,    10]\n","2024-02-11 17:02:16,231 [INFO] loss: 0.0037    [24,    20]\n","2024-02-11 17:02:16,251 [INFO] loss: 0.0035    [25,    10]\n","2024-02-11 17:02:16,278 [INFO] loss: 0.0034    [25,    20]\n","2024-02-11 17:02:16,302 [INFO] loss: 0.0026    [26,    10]\n","2024-02-11 17:02:16,322 [INFO] loss: 0.0030    [26,    20]\n","2024-02-11 17:02:16,347 [INFO] loss: 0.0027    [27,    10]\n","2024-02-11 17:02:16,371 [INFO] loss: 0.0028    [27,    20]\n","2024-02-11 17:02:16,395 [INFO] loss: 0.0029    [28,    10]\n","2024-02-11 17:02:16,419 [INFO] loss: 0.0027    [28,    20]\n","2024-02-11 17:02:16,449 [INFO] loss: 0.0027    [29,    10]\n","2024-02-11 17:02:16,473 [INFO] loss: 0.0029    [29,    20]\n","2024-02-11 17:02:16,498 [INFO] loss: 0.0032    [30,    10]\n","2024-02-11 17:02:16,522 [INFO] loss: 0.0025    [30,    20]\n","2024-02-11 17:02:16,545 [INFO] loss: 0.0031    [31,    10]\n","2024-02-11 17:02:16,567 [INFO] loss: 0.0024    [31,    20]\n","2024-02-11 17:02:16,590 [INFO] loss: 0.0028    [32,    10]\n","2024-02-11 17:02:16,617 [INFO] loss: 0.0037    [32,    20]\n","2024-02-11 17:02:16,640 [INFO] loss: 0.0026    [33,    10]\n","2024-02-11 17:02:16,662 [INFO] loss: 0.0041    [33,    20]\n","2024-02-11 17:02:16,684 [INFO] loss: 0.0025    [34,    10]\n","2024-02-11 17:02:16,706 [INFO] loss: 0.0024    [34,    20]\n","2024-02-11 17:02:16,728 [INFO] loss: 0.0023    [35,    10]\n","2024-02-11 17:02:16,748 [INFO] loss: 0.0022    [35,    20]\n","2024-02-11 17:02:16,773 [INFO] loss: 0.0020    [36,    10]\n","2024-02-11 17:02:16,795 [INFO] loss: 0.0025    [36,    20]\n","2024-02-11 17:02:16,818 [INFO] loss: 0.0024    [37,    10]\n","2024-02-11 17:02:16,844 [INFO] loss: 0.0027    [37,    20]\n","2024-02-11 17:02:16,865 [INFO] loss: 0.0023    [38,    10]\n","2024-02-11 17:02:16,889 [INFO] loss: 0.0032    [38,    20]\n","2024-02-11 17:02:16,911 [INFO] loss: 0.0022    [39,    10]\n","2024-02-11 17:02:16,931 [INFO] loss: 0.0019    [39,    20]\n","2024-02-11 17:02:16,956 [INFO] loss: 0.0021    [40,    10]\n","2024-02-11 17:02:16,978 [INFO] loss: 0.0014    [40,    20]\n","2024-02-11 17:02:16,999 [INFO] loss: 0.0009    [41,    10]\n","2024-02-11 17:02:17,018 [INFO] loss: 0.0019    [41,    20]\n","2024-02-11 17:02:17,039 [INFO] loss: 0.0016    [42,    10]\n","2024-02-11 17:02:17,060 [INFO] loss: 0.0022    [42,    20]\n","2024-02-11 17:02:17,082 [INFO] loss: 0.0015    [43,    10]\n","2024-02-11 17:02:17,102 [INFO] loss: 0.0021    [43,    20]\n","2024-02-11 17:02:17,125 [INFO] loss: 0.0016    [44,    10]\n","2024-02-11 17:02:17,146 [INFO] loss: 0.0019    [44,    20]\n","2024-02-11 17:02:17,168 [INFO] loss: 0.0018    [45,    10]\n","2024-02-11 17:02:17,189 [INFO] loss: 0.0017    [45,    20]\n","2024-02-11 17:02:17,211 [INFO] loss: 0.0017    [46,    10]\n","2024-02-11 17:02:17,231 [INFO] loss: 0.0021    [46,    20]\n","2024-02-11 17:02:17,253 [INFO] loss: 0.0022    [47,    10]\n","2024-02-11 17:02:17,273 [INFO] loss: 0.0021    [47,    20]\n","2024-02-11 17:02:17,295 [INFO] loss: 0.0021    [48,    10]\n","2024-02-11 17:02:17,321 [INFO] loss: 0.0015    [48,    20]\n","2024-02-11 17:02:17,345 [INFO] loss: 0.0010    [49,    10]\n","2024-02-11 17:02:17,370 [INFO] loss: 0.0018    [49,    20]\n","2024-02-11 17:02:17,394 [INFO] loss: 0.0021    [50,    10]\n","2024-02-11 17:02:17,419 [INFO] loss: 0.0023    [50,    20]\n","2024-02-11 17:02:17,493 [INFO] Result on Train Data : {'AUC': 0.9998782603402623, 'ACC': 0.9888535031847133, 'F1 Score': 0.9888471403672123, 'AUPR': 0, 'Loss': 0.04057357320562005}\n"]}],"id":"OqKrF7HmNFx1"},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"0eQGNWm_NMVG"},"id":"0eQGNWm_NMVG"},{"cell_type":"code","source":["test_result = SimpleTester().test(model=mda_classifier,\n","                                  data=test_data,\n","                                  config=classifier_optimizer_config)"],"metadata":{"id":"U05mXL_fNHpG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670938309,"user_tz":-210,"elapsed":13,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"0b9e4e82-6874-4f78-e925-a3f11eeabf77"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 17:02:17,505 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:17,511 [INFO] moving data and model to cpu\n","2024-02-11 17:02:17,557 [INFO] Result on Test Data : {'AUC': 0.9718271184579055, 'ACC': 0.9111111111111111, 'F1 Score': 0.9110671936758893, 'AUPR': 0, 'Loss': 0.2504637518690692}\n"]}],"id":"U05mXL_fNHpG"},{"cell_type":"code","source":["test_result.get_result()"],"metadata":{"id":"oqgiZQqRWWGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670938309,"user_tz":-210,"elapsed":10,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"07254272-b76a-442f-ffc1-52c52e3e581d"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AUC': 0.9718271184579055,\n"," 'ACC': 0.9111111111111111,\n"," 'F1 Score': 0.9110671936758893,\n"," 'AUPR': 0,\n"," 'Loss': 0.2504637518690692}"]},"metadata":{},"execution_count":19}],"id":"oqgiZQqRWWGF"},{"cell_type":"markdown","source":["## Cross Validation"],"metadata":{"id":"ti8vEX_cNNwy"},"id":"ti8vEX_cNNwy"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707670948640,"user_tz":-210,"elapsed":10340,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"e0852035-7b86-4a04-c263-a41544541e6c","id":"sfI286uv6o-e"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 17:02:17,580 [INFO] Initializing SimpleMDAClassifierFactory with model : simple classifier\n","2024-02-11 17:02:17,582 [INFO] Initializing SimplePytorchDataTrainTestSplit\n","2024-02-11 17:02:17,584 [INFO] Start 5-fold Cross Validation with config : adam optimizer\n","2024-02-11 17:02:17,587 [INFO] ---- Fold 1 ----\n","2024-02-11 17:02:17,590 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 17:02:17,593 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 17:02:17,595 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:17,596 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 17:02:17,598 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:17,600 [INFO] moving data and model to cpu\n","2024-02-11 17:02:17,632 [INFO] loss: 0.0185    [1,    10]\n","2024-02-11 17:02:17,661 [INFO] loss: 0.0147    [1,    20]\n","2024-02-11 17:02:17,692 [INFO] loss: 0.0158    [2,    10]\n","2024-02-11 17:02:17,713 [INFO] loss: 0.0125    [2,    20]\n","2024-02-11 17:02:17,737 [INFO] loss: 0.0128    [3,    10]\n","2024-02-11 17:02:17,760 [INFO] loss: 0.0118    [3,    20]\n","2024-02-11 17:02:17,789 [INFO] loss: 0.0118    [4,    10]\n","2024-02-11 17:02:17,816 [INFO] loss: 0.0096    [4,    20]\n","2024-02-11 17:02:17,841 [INFO] loss: 0.0100    [5,    10]\n","2024-02-11 17:02:17,864 [INFO] loss: 0.0090    [5,    20]\n","2024-02-11 17:02:17,897 [INFO] loss: 0.0102    [6,    10]\n","2024-02-11 17:02:17,917 [INFO] loss: 0.0094    [6,    20]\n","2024-02-11 17:02:17,941 [INFO] loss: 0.0081    [7,    10]\n","2024-02-11 17:02:17,960 [INFO] loss: 0.0099    [7,    20]\n","2024-02-11 17:02:17,991 [INFO] loss: 0.0075    [8,    10]\n","2024-02-11 17:02:18,010 [INFO] loss: 0.0077    [8,    20]\n","2024-02-11 17:02:18,042 [INFO] loss: 0.0063    [9,    10]\n","2024-02-11 17:02:18,063 [INFO] loss: 0.0083    [9,    20]\n","2024-02-11 17:02:18,088 [INFO] loss: 0.0069    [10,    10]\n","2024-02-11 17:02:18,107 [INFO] loss: 0.0075    [10,    20]\n","2024-02-11 17:02:18,135 [INFO] loss: 0.0072    [11,    10]\n","2024-02-11 17:02:18,154 [INFO] loss: 0.0058    [11,    20]\n","2024-02-11 17:02:18,180 [INFO] loss: 0.0062    [12,    10]\n","2024-02-11 17:02:18,199 [INFO] loss: 0.0069    [12,    20]\n","2024-02-11 17:02:18,226 [INFO] loss: 0.0057    [13,    10]\n","2024-02-11 17:02:18,246 [INFO] loss: 0.0062    [13,    20]\n","2024-02-11 17:02:18,270 [INFO] loss: 0.0050    [14,    10]\n","2024-02-11 17:02:18,290 [INFO] loss: 0.0057    [14,    20]\n","2024-02-11 17:02:18,314 [INFO] loss: 0.0061    [15,    10]\n","2024-02-11 17:02:18,333 [INFO] loss: 0.0047    [15,    20]\n","2024-02-11 17:02:18,363 [INFO] loss: 0.0043    [16,    10]\n","2024-02-11 17:02:18,386 [INFO] loss: 0.0055    [16,    20]\n","2024-02-11 17:02:18,413 [INFO] loss: 0.0044    [17,    10]\n","2024-02-11 17:02:18,433 [INFO] loss: 0.0043    [17,    20]\n","2024-02-11 17:02:18,458 [INFO] loss: 0.0043    [18,    10]\n","2024-02-11 17:02:18,477 [INFO] loss: 0.0037    [18,    20]\n","2024-02-11 17:02:18,506 [INFO] loss: 0.0039    [19,    10]\n","2024-02-11 17:02:18,528 [INFO] loss: 0.0035    [19,    20]\n","2024-02-11 17:02:18,561 [INFO] loss: 0.0040    [20,    10]\n","2024-02-11 17:02:18,584 [INFO] loss: 0.0040    [20,    20]\n","2024-02-11 17:02:18,614 [INFO] loss: 0.0037    [21,    10]\n","2024-02-11 17:02:18,635 [INFO] loss: 0.0046    [21,    20]\n","2024-02-11 17:02:18,660 [INFO] loss: 0.0031    [22,    10]\n","2024-02-11 17:02:18,679 [INFO] loss: 0.0042    [22,    20]\n","2024-02-11 17:02:18,703 [INFO] loss: 0.0034    [23,    10]\n","2024-02-11 17:02:18,723 [INFO] loss: 0.0037    [23,    20]\n","2024-02-11 17:02:18,747 [INFO] loss: 0.0027    [24,    10]\n","2024-02-11 17:02:18,766 [INFO] loss: 0.0029    [24,    20]\n","2024-02-11 17:02:18,790 [INFO] loss: 0.0021    [25,    10]\n","2024-02-11 17:02:18,810 [INFO] loss: 0.0029    [25,    20]\n","2024-02-11 17:02:18,836 [INFO] loss: 0.0025    [26,    10]\n","2024-02-11 17:02:18,856 [INFO] loss: 0.0024    [26,    20]\n","2024-02-11 17:02:18,884 [INFO] loss: 0.0021    [27,    10]\n","2024-02-11 17:02:18,903 [INFO] loss: 0.0022    [27,    20]\n","2024-02-11 17:02:18,931 [INFO] loss: 0.0021    [28,    10]\n","2024-02-11 17:02:18,952 [INFO] loss: 0.0031    [28,    20]\n","2024-02-11 17:02:18,981 [INFO] loss: 0.0024    [29,    10]\n","2024-02-11 17:02:19,001 [INFO] loss: 0.0026    [29,    20]\n","2024-02-11 17:02:19,027 [INFO] loss: 0.0019    [30,    10]\n","2024-02-11 17:02:19,050 [INFO] loss: 0.0033    [30,    20]\n","2024-02-11 17:02:19,080 [INFO] loss: 0.0024    [31,    10]\n","2024-02-11 17:02:19,115 [INFO] loss: 0.0033    [31,    20]\n","2024-02-11 17:02:19,149 [INFO] loss: 0.0018    [32,    10]\n","2024-02-11 17:02:19,174 [INFO] loss: 0.0025    [32,    20]\n","2024-02-11 17:02:19,202 [INFO] loss: 0.0016    [33,    10]\n","2024-02-11 17:02:19,223 [INFO] loss: 0.0024    [33,    20]\n","2024-02-11 17:02:19,249 [INFO] loss: 0.0018    [34,    10]\n","2024-02-11 17:02:19,269 [INFO] loss: 0.0018    [34,    20]\n","2024-02-11 17:02:19,295 [INFO] loss: 0.0015    [35,    10]\n","2024-02-11 17:02:19,319 [INFO] loss: 0.0020    [35,    20]\n","2024-02-11 17:02:19,349 [INFO] loss: 0.0012    [36,    10]\n","2024-02-11 17:02:19,373 [INFO] loss: 0.0012    [36,    20]\n","2024-02-11 17:02:19,412 [INFO] loss: 0.0015    [37,    10]\n","2024-02-11 17:02:19,442 [INFO] loss: 0.0012    [37,    20]\n","2024-02-11 17:02:19,473 [INFO] loss: 0.0009    [38,    10]\n","2024-02-11 17:02:19,502 [INFO] loss: 0.0019    [38,    20]\n","2024-02-11 17:02:19,538 [INFO] loss: 0.0014    [39,    10]\n","2024-02-11 17:02:19,567 [INFO] loss: 0.0014    [39,    20]\n","2024-02-11 17:02:19,595 [INFO] loss: 0.0017    [40,    10]\n","2024-02-11 17:02:19,624 [INFO] loss: 0.0018    [40,    20]\n","2024-02-11 17:02:19,658 [INFO] loss: 0.0012    [41,    10]\n","2024-02-11 17:02:19,682 [INFO] loss: 0.0009    [41,    20]\n","2024-02-11 17:02:19,718 [INFO] loss: 0.0011    [42,    10]\n","2024-02-11 17:02:19,744 [INFO] loss: 0.0013    [42,    20]\n","2024-02-11 17:02:19,775 [INFO] loss: 0.0024    [43,    10]\n","2024-02-11 17:02:19,799 [INFO] loss: 0.0012    [43,    20]\n","2024-02-11 17:02:19,827 [INFO] loss: 0.0013    [44,    10]\n","2024-02-11 17:02:19,854 [INFO] loss: 0.0015    [44,    20]\n","2024-02-11 17:02:19,882 [INFO] loss: 0.0015    [45,    10]\n","2024-02-11 17:02:19,905 [INFO] loss: 0.0024    [45,    20]\n","2024-02-11 17:02:19,939 [INFO] loss: 0.0015    [46,    10]\n","2024-02-11 17:02:19,962 [INFO] loss: 0.0012    [46,    20]\n","2024-02-11 17:02:19,990 [INFO] loss: 0.0016    [47,    10]\n","2024-02-11 17:02:20,017 [INFO] loss: 0.0018    [47,    20]\n","2024-02-11 17:02:20,050 [INFO] loss: 0.0009    [48,    10]\n","2024-02-11 17:02:20,067 [INFO] loss: 0.0013    [48,    20]\n","2024-02-11 17:02:20,090 [INFO] loss: 0.0007    [49,    10]\n","2024-02-11 17:02:20,107 [INFO] loss: 0.0015    [49,    20]\n","2024-02-11 17:02:20,127 [INFO] loss: 0.0009    [50,    10]\n","2024-02-11 17:02:20,144 [INFO] loss: 0.0007    [50,    20]\n","2024-02-11 17:02:20,187 [INFO] Result on Train Data : {'AUC': 0.9999922599421043, 'ACC': 0.9972183588317107, 'F1 Score': 0.997217147634382, 'AUPR': 0, 'Loss': 0.013847195052379824}\n","2024-02-11 17:02:20,189 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:20,191 [INFO] moving data and model to cpu\n","2024-02-11 17:02:20,212 [INFO] Result on Test Data : {'AUC': 0.8550451807228916, 'ACC': 0.7541899441340782, 'F1 Score': 0.7538134533633409, 'AUPR': 0, 'Loss': 1.0913785894711812}\n","2024-02-11 17:02:20,213 [INFO] Result of fold 1 : {'AUC': 0.8550451807228916, 'ACC': 0.7541899441340782, 'F1 Score': 0.7538134533633409, 'AUPR': 0, 'Loss': 1.0913785894711812}\n","2024-02-11 17:02:20,216 [INFO] ---- Fold 2 ----\n","2024-02-11 17:02:20,220 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 17:02:20,223 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 17:02:20,225 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:20,227 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 17:02:20,230 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:20,231 [INFO] moving data and model to cpu\n","2024-02-11 17:02:20,253 [INFO] loss: 0.0185    [1,    10]\n","2024-02-11 17:02:20,269 [INFO] loss: 0.0148    [1,    20]\n","2024-02-11 17:02:20,290 [INFO] loss: 0.0140    [2,    10]\n","2024-02-11 17:02:20,307 [INFO] loss: 0.0127    [2,    20]\n","2024-02-11 17:02:20,326 [INFO] loss: 0.0115    [3,    10]\n","2024-02-11 17:02:20,344 [INFO] loss: 0.0117    [3,    20]\n","2024-02-11 17:02:20,364 [INFO] loss: 0.0109    [4,    10]\n","2024-02-11 17:02:20,380 [INFO] loss: 0.0105    [4,    20]\n","2024-02-11 17:02:20,400 [INFO] loss: 0.0088    [5,    10]\n","2024-02-11 17:02:20,418 [INFO] loss: 0.0104    [5,    20]\n","2024-02-11 17:02:20,439 [INFO] loss: 0.0099    [6,    10]\n","2024-02-11 17:02:20,457 [INFO] loss: 0.0092    [6,    20]\n","2024-02-11 17:02:20,476 [INFO] loss: 0.0079    [7,    10]\n","2024-02-11 17:02:20,493 [INFO] loss: 0.0099    [7,    20]\n","2024-02-11 17:02:20,513 [INFO] loss: 0.0073    [8,    10]\n","2024-02-11 17:02:20,530 [INFO] loss: 0.0082    [8,    20]\n","2024-02-11 17:02:20,550 [INFO] loss: 0.0075    [9,    10]\n","2024-02-11 17:02:20,567 [INFO] loss: 0.0088    [9,    20]\n","2024-02-11 17:02:20,587 [INFO] loss: 0.0078    [10,    10]\n","2024-02-11 17:02:20,605 [INFO] loss: 0.0078    [10,    20]\n","2024-02-11 17:02:20,629 [INFO] loss: 0.0069    [11,    10]\n","2024-02-11 17:02:20,652 [INFO] loss: 0.0075    [11,    20]\n","2024-02-11 17:02:20,687 [INFO] loss: 0.0058    [12,    10]\n","2024-02-11 17:02:20,711 [INFO] loss: 0.0076    [12,    20]\n","2024-02-11 17:02:20,731 [INFO] loss: 0.0068    [13,    10]\n","2024-02-11 17:02:20,749 [INFO] loss: 0.0055    [13,    20]\n","2024-02-11 17:02:20,769 [INFO] loss: 0.0056    [14,    10]\n","2024-02-11 17:02:20,785 [INFO] loss: 0.0057    [14,    20]\n","2024-02-11 17:02:20,805 [INFO] loss: 0.0048    [15,    10]\n","2024-02-11 17:02:20,821 [INFO] loss: 0.0058    [15,    20]\n","2024-02-11 17:02:20,841 [INFO] loss: 0.0043    [16,    10]\n","2024-02-11 17:02:20,859 [INFO] loss: 0.0057    [16,    20]\n","2024-02-11 17:02:20,879 [INFO] loss: 0.0045    [17,    10]\n","2024-02-11 17:02:20,895 [INFO] loss: 0.0053    [17,    20]\n","2024-02-11 17:02:20,915 [INFO] loss: 0.0041    [18,    10]\n","2024-02-11 17:02:20,932 [INFO] loss: 0.0054    [18,    20]\n","2024-02-11 17:02:20,953 [INFO] loss: 0.0031    [19,    10]\n","2024-02-11 17:02:20,972 [INFO] loss: 0.0052    [19,    20]\n","2024-02-11 17:02:20,993 [INFO] loss: 0.0035    [20,    10]\n","2024-02-11 17:02:21,015 [INFO] loss: 0.0037    [20,    20]\n","2024-02-11 17:02:21,036 [INFO] loss: 0.0032    [21,    10]\n","2024-02-11 17:02:21,053 [INFO] loss: 0.0037    [21,    20]\n","2024-02-11 17:02:21,072 [INFO] loss: 0.0032    [22,    10]\n","2024-02-11 17:02:21,092 [INFO] loss: 0.0035    [22,    20]\n","2024-02-11 17:02:21,114 [INFO] loss: 0.0027    [23,    10]\n","2024-02-11 17:02:21,130 [INFO] loss: 0.0037    [23,    20]\n","2024-02-11 17:02:21,151 [INFO] loss: 0.0043    [24,    10]\n","2024-02-11 17:02:21,167 [INFO] loss: 0.0046    [24,    20]\n","2024-02-11 17:02:21,186 [INFO] loss: 0.0033    [25,    10]\n","2024-02-11 17:02:21,203 [INFO] loss: 0.0027    [25,    20]\n","2024-02-11 17:02:21,223 [INFO] loss: 0.0029    [26,    10]\n","2024-02-11 17:02:21,239 [INFO] loss: 0.0036    [26,    20]\n","2024-02-11 17:02:21,259 [INFO] loss: 0.0026    [27,    10]\n","2024-02-11 17:02:21,276 [INFO] loss: 0.0024    [27,    20]\n","2024-02-11 17:02:21,305 [INFO] loss: 0.0023    [28,    10]\n","2024-02-11 17:02:21,321 [INFO] loss: 0.0022    [28,    20]\n","2024-02-11 17:02:21,344 [INFO] loss: 0.0017    [29,    10]\n","2024-02-11 17:02:21,365 [INFO] loss: 0.0021    [29,    20]\n","2024-02-11 17:02:21,386 [INFO] loss: 0.0026    [30,    10]\n","2024-02-11 17:02:21,402 [INFO] loss: 0.0018    [30,    20]\n","2024-02-11 17:02:21,421 [INFO] loss: 0.0021    [31,    10]\n","2024-02-11 17:02:21,437 [INFO] loss: 0.0021    [31,    20]\n","2024-02-11 17:02:21,459 [INFO] loss: 0.0020    [32,    10]\n","2024-02-11 17:02:21,478 [INFO] loss: 0.0024    [32,    20]\n","2024-02-11 17:02:21,498 [INFO] loss: 0.0016    [33,    10]\n","2024-02-11 17:02:21,514 [INFO] loss: 0.0036    [33,    20]\n","2024-02-11 17:02:21,535 [INFO] loss: 0.0023    [34,    10]\n","2024-02-11 17:02:21,551 [INFO] loss: 0.0024    [34,    20]\n","2024-02-11 17:02:21,571 [INFO] loss: 0.0031    [35,    10]\n","2024-02-11 17:02:21,586 [INFO] loss: 0.0024    [35,    20]\n","2024-02-11 17:02:21,607 [INFO] loss: 0.0012    [36,    10]\n","2024-02-11 17:02:21,623 [INFO] loss: 0.0020    [36,    20]\n","2024-02-11 17:02:21,643 [INFO] loss: 0.0019    [37,    10]\n","2024-02-11 17:02:21,659 [INFO] loss: 0.0014    [37,    20]\n","2024-02-11 17:02:21,678 [INFO] loss: 0.0014    [38,    10]\n","2024-02-11 17:02:21,694 [INFO] loss: 0.0015    [38,    20]\n","2024-02-11 17:02:21,721 [INFO] loss: 0.0013    [39,    10]\n","2024-02-11 17:02:21,740 [INFO] loss: 0.0015    [39,    20]\n","2024-02-11 17:02:21,761 [INFO] loss: 0.0018    [40,    10]\n","2024-02-11 17:02:21,778 [INFO] loss: 0.0020    [40,    20]\n","2024-02-11 17:02:21,798 [INFO] loss: 0.0015    [41,    10]\n","2024-02-11 17:02:21,814 [INFO] loss: 0.0024    [41,    20]\n","2024-02-11 17:02:21,835 [INFO] loss: 0.0022    [42,    10]\n","2024-02-11 17:02:21,852 [INFO] loss: 0.0019    [42,    20]\n","2024-02-11 17:02:21,875 [INFO] loss: 0.0018    [43,    10]\n","2024-02-11 17:02:21,891 [INFO] loss: 0.0017    [43,    20]\n","2024-02-11 17:02:21,915 [INFO] loss: 0.0012    [44,    10]\n","2024-02-11 17:02:21,932 [INFO] loss: 0.0018    [44,    20]\n","2024-02-11 17:02:21,954 [INFO] loss: 0.0012    [45,    10]\n","2024-02-11 17:02:21,971 [INFO] loss: 0.0017    [45,    20]\n","2024-02-11 17:02:21,994 [INFO] loss: 0.0013    [46,    10]\n","2024-02-11 17:02:22,010 [INFO] loss: 0.0009    [46,    20]\n","2024-02-11 17:02:22,031 [INFO] loss: 0.0011    [47,    10]\n","2024-02-11 17:02:22,048 [INFO] loss: 0.0014    [47,    20]\n","2024-02-11 17:02:22,068 [INFO] loss: 0.0011    [48,    10]\n","2024-02-11 17:02:22,084 [INFO] loss: 0.0010    [48,    20]\n","2024-02-11 17:02:22,104 [INFO] loss: 0.0008    [49,    10]\n","2024-02-11 17:02:22,120 [INFO] loss: 0.0009    [49,    20]\n","2024-02-11 17:02:22,141 [INFO] loss: 0.0013    [50,    10]\n","2024-02-11 17:02:22,157 [INFO] loss: 0.0012    [50,    20]\n","2024-02-11 17:02:22,199 [INFO] Result on Train Data : {'AUC': 0.999566630552546, 'ACC': 0.9944367176634215, 'F1 Score': 0.9944364486125943, 'AUPR': 0, 'Loss': 0.025786032839475767}\n","2024-02-11 17:02:22,200 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:22,203 [INFO] moving data and model to cpu\n","2024-02-11 17:02:22,224 [INFO] Result on Test Data : {'AUC': 0.8813516896120149, 'ACC': 0.770949720670391, 'F1 Score': 0.7686098937478325, 'AUPR': 0, 'Loss': 0.9281674871842066}\n","2024-02-11 17:02:22,225 [INFO] Result of fold 2 : {'AUC': 0.8813516896120149, 'ACC': 0.770949720670391, 'F1 Score': 0.7686098937478325, 'AUPR': 0, 'Loss': 0.9281674871842066}\n","2024-02-11 17:02:22,228 [INFO] ---- Fold 3 ----\n","2024-02-11 17:02:22,230 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 17:02:22,233 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 17:02:22,234 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:22,235 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 17:02:22,241 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:22,242 [INFO] moving data and model to cpu\n","2024-02-11 17:02:22,264 [INFO] loss: 0.0192    [1,    10]\n","2024-02-11 17:02:22,280 [INFO] loss: 0.0147    [1,    20]\n","2024-02-11 17:02:22,304 [INFO] loss: 0.0131    [2,    10]\n","2024-02-11 17:02:22,320 [INFO] loss: 0.0136    [2,    20]\n","2024-02-11 17:02:22,340 [INFO] loss: 0.0107    [3,    10]\n","2024-02-11 17:02:22,356 [INFO] loss: 0.0129    [3,    20]\n","2024-02-11 17:02:22,375 [INFO] loss: 0.0111    [4,    10]\n","2024-02-11 17:02:22,391 [INFO] loss: 0.0104    [4,    20]\n","2024-02-11 17:02:22,411 [INFO] loss: 0.0096    [5,    10]\n","2024-02-11 17:02:22,428 [INFO] loss: 0.0100    [5,    20]\n","2024-02-11 17:02:22,448 [INFO] loss: 0.0093    [6,    10]\n","2024-02-11 17:02:22,466 [INFO] loss: 0.0084    [6,    20]\n","2024-02-11 17:02:22,485 [INFO] loss: 0.0094    [7,    10]\n","2024-02-11 17:02:22,502 [INFO] loss: 0.0079    [7,    20]\n","2024-02-11 17:02:22,527 [INFO] loss: 0.0066    [8,    10]\n","2024-02-11 17:02:22,543 [INFO] loss: 0.0088    [8,    20]\n","2024-02-11 17:02:22,564 [INFO] loss: 0.0091    [9,    10]\n","2024-02-11 17:02:22,581 [INFO] loss: 0.0070    [9,    20]\n","2024-02-11 17:02:22,602 [INFO] loss: 0.0069    [10,    10]\n","2024-02-11 17:02:22,618 [INFO] loss: 0.0073    [10,    20]\n","2024-02-11 17:02:22,639 [INFO] loss: 0.0074    [11,    10]\n","2024-02-11 17:02:22,654 [INFO] loss: 0.0054    [11,    20]\n","2024-02-11 17:02:22,675 [INFO] loss: 0.0059    [12,    10]\n","2024-02-11 17:02:22,692 [INFO] loss: 0.0061    [12,    20]\n","2024-02-11 17:02:22,715 [INFO] loss: 0.0051    [13,    10]\n","2024-02-11 17:02:22,731 [INFO] loss: 0.0057    [13,    20]\n","2024-02-11 17:02:22,765 [INFO] loss: 0.0061    [14,    10]\n","2024-02-11 17:02:22,781 [INFO] loss: 0.0052    [14,    20]\n","2024-02-11 17:02:22,801 [INFO] loss: 0.0051    [15,    10]\n","2024-02-11 17:02:22,817 [INFO] loss: 0.0069    [15,    20]\n","2024-02-11 17:02:22,837 [INFO] loss: 0.0062    [16,    10]\n","2024-02-11 17:02:22,854 [INFO] loss: 0.0052    [16,    20]\n","2024-02-11 17:02:22,874 [INFO] loss: 0.0044    [17,    10]\n","2024-02-11 17:02:22,890 [INFO] loss: 0.0046    [17,    20]\n","2024-02-11 17:02:22,910 [INFO] loss: 0.0029    [18,    10]\n","2024-02-11 17:02:22,926 [INFO] loss: 0.0039    [18,    20]\n","2024-02-11 17:02:22,946 [INFO] loss: 0.0037    [19,    10]\n","2024-02-11 17:02:22,961 [INFO] loss: 0.0042    [19,    20]\n","2024-02-11 17:02:22,981 [INFO] loss: 0.0031    [20,    10]\n","2024-02-11 17:02:22,998 [INFO] loss: 0.0028    [20,    20]\n","2024-02-11 17:02:23,019 [INFO] loss: 0.0026    [21,    10]\n","2024-02-11 17:02:23,035 [INFO] loss: 0.0033    [21,    20]\n","2024-02-11 17:02:23,056 [INFO] loss: 0.0031    [22,    10]\n","2024-02-11 17:02:23,073 [INFO] loss: 0.0035    [22,    20]\n","2024-02-11 17:02:23,095 [INFO] loss: 0.0026    [23,    10]\n","2024-02-11 17:02:23,111 [INFO] loss: 0.0027    [23,    20]\n","2024-02-11 17:02:23,135 [INFO] loss: 0.0023    [24,    10]\n","2024-02-11 17:02:23,151 [INFO] loss: 0.0018    [24,    20]\n","2024-02-11 17:02:23,172 [INFO] loss: 0.0022    [25,    10]\n","2024-02-11 17:02:23,188 [INFO] loss: 0.0028    [25,    20]\n","2024-02-11 17:02:23,208 [INFO] loss: 0.0036    [26,    10]\n","2024-02-11 17:02:23,224 [INFO] loss: 0.0023    [26,    20]\n","2024-02-11 17:02:23,244 [INFO] loss: 0.0022    [27,    10]\n","2024-02-11 17:02:23,260 [INFO] loss: 0.0022    [27,    20]\n","2024-02-11 17:02:23,280 [INFO] loss: 0.0021    [28,    10]\n","2024-02-11 17:02:23,298 [INFO] loss: 0.0023    [28,    20]\n","2024-02-11 17:02:23,317 [INFO] loss: 0.0014    [29,    10]\n","2024-02-11 17:02:23,333 [INFO] loss: 0.0030    [29,    20]\n","2024-02-11 17:02:23,356 [INFO] loss: 0.0023    [30,    10]\n","2024-02-11 17:02:23,373 [INFO] loss: 0.0017    [30,    20]\n","2024-02-11 17:02:23,393 [INFO] loss: 0.0023    [31,    10]\n","2024-02-11 17:02:23,409 [INFO] loss: 0.0023    [31,    20]\n","2024-02-11 17:02:23,432 [INFO] loss: 0.0012    [32,    10]\n","2024-02-11 17:02:23,449 [INFO] loss: 0.0027    [32,    20]\n","2024-02-11 17:02:23,473 [INFO] loss: 0.0012    [33,    10]\n","2024-02-11 17:02:23,489 [INFO] loss: 0.0030    [33,    20]\n","2024-02-11 17:02:23,510 [INFO] loss: 0.0027    [34,    10]\n","2024-02-11 17:02:23,528 [INFO] loss: 0.0029    [34,    20]\n","2024-02-11 17:02:23,548 [INFO] loss: 0.0018    [35,    10]\n","2024-02-11 17:02:23,566 [INFO] loss: 0.0020    [35,    20]\n","2024-02-11 17:02:23,586 [INFO] loss: 0.0016    [36,    10]\n","2024-02-11 17:02:23,603 [INFO] loss: 0.0023    [36,    20]\n","2024-02-11 17:02:23,623 [INFO] loss: 0.0021    [37,    10]\n","2024-02-11 17:02:23,640 [INFO] loss: 0.0023    [37,    20]\n","2024-02-11 17:02:23,660 [INFO] loss: 0.0018    [38,    10]\n","2024-02-11 17:02:23,678 [INFO] loss: 0.0018    [38,    20]\n","2024-02-11 17:02:23,698 [INFO] loss: 0.0015    [39,    10]\n","2024-02-11 17:02:23,715 [INFO] loss: 0.0032    [39,    20]\n","2024-02-11 17:02:23,736 [INFO] loss: 0.0020    [40,    10]\n","2024-02-11 17:02:23,753 [INFO] loss: 0.0019    [40,    20]\n","2024-02-11 17:02:23,780 [INFO] loss: 0.0012    [41,    10]\n","2024-02-11 17:02:23,799 [INFO] loss: 0.0018    [41,    20]\n","2024-02-11 17:02:23,819 [INFO] loss: 0.0017    [42,    10]\n","2024-02-11 17:02:23,835 [INFO] loss: 0.0012    [42,    20]\n","2024-02-11 17:02:23,858 [INFO] loss: 0.0012    [43,    10]\n","2024-02-11 17:02:23,875 [INFO] loss: 0.0010    [43,    20]\n","2024-02-11 17:02:23,896 [INFO] loss: 0.0010    [44,    10]\n","2024-02-11 17:02:23,915 [INFO] loss: 0.0013    [44,    20]\n","2024-02-11 17:02:23,936 [INFO] loss: 0.0014    [45,    10]\n","2024-02-11 17:02:23,952 [INFO] loss: 0.0020    [45,    20]\n","2024-02-11 17:02:23,973 [INFO] loss: 0.0016    [46,    10]\n","2024-02-11 17:02:23,989 [INFO] loss: 0.0011    [46,    20]\n","2024-02-11 17:02:24,009 [INFO] loss: 0.0019    [47,    10]\n","2024-02-11 17:02:24,028 [INFO] loss: 0.0014    [47,    20]\n","2024-02-11 17:02:24,048 [INFO] loss: 0.0008    [48,    10]\n","2024-02-11 17:02:24,070 [INFO] loss: 0.0011    [48,    20]\n","2024-02-11 17:02:24,092 [INFO] loss: 0.0007    [49,    10]\n","2024-02-11 17:02:24,109 [INFO] loss: 0.0009    [49,    20]\n","2024-02-11 17:02:24,130 [INFO] loss: 0.0006    [50,    10]\n","2024-02-11 17:02:24,146 [INFO] loss: 0.0008    [50,    20]\n","2024-02-11 17:02:24,188 [INFO] Result on Train Data : {'AUC': 0.9999922620982095, 'ACC': 0.9986091794158554, 'F1 Score': 0.9986090825554964, 'AUPR': 0, 'Loss': 0.01368810602909197}\n","2024-02-11 17:02:24,190 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:24,193 [INFO] moving data and model to cpu\n","2024-02-11 17:02:24,216 [INFO] Result on Test Data : {'AUC': 0.8732508745627187, 'ACC': 0.7932960893854749, 'F1 Score': 0.7932702812373194, 'AUPR': 0, 'Loss': 1.2546625634034474}\n","2024-02-11 17:02:24,217 [INFO] Result of fold 3 : {'AUC': 0.8732508745627187, 'ACC': 0.7932960893854749, 'F1 Score': 0.7932702812373194, 'AUPR': 0, 'Loss': 1.2546625634034474}\n","2024-02-11 17:02:24,220 [INFO] ---- Fold 4 ----\n","2024-02-11 17:02:24,223 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 17:02:24,226 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 17:02:24,228 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:24,229 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 17:02:24,231 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:24,233 [INFO] moving data and model to cpu\n","2024-02-11 17:02:24,255 [INFO] loss: 0.0196    [1,    10]\n","2024-02-11 17:02:24,273 [INFO] loss: 0.0156    [1,    20]\n","2024-02-11 17:02:24,298 [INFO] loss: 0.0139    [2,    10]\n","2024-02-11 17:02:24,318 [INFO] loss: 0.0135    [2,    20]\n","2024-02-11 17:02:24,338 [INFO] loss: 0.0121    [3,    10]\n","2024-02-11 17:02:24,355 [INFO] loss: 0.0129    [3,    20]\n","2024-02-11 17:02:24,375 [INFO] loss: 0.0113    [4,    10]\n","2024-02-11 17:02:24,392 [INFO] loss: 0.0114    [4,    20]\n","2024-02-11 17:02:24,412 [INFO] loss: 0.0108    [5,    10]\n","2024-02-11 17:02:24,430 [INFO] loss: 0.0117    [5,    20]\n","2024-02-11 17:02:24,451 [INFO] loss: 0.0104    [6,    10]\n","2024-02-11 17:02:24,470 [INFO] loss: 0.0104    [6,    20]\n","2024-02-11 17:02:24,491 [INFO] loss: 0.0087    [7,    10]\n","2024-02-11 17:02:24,511 [INFO] loss: 0.0085    [7,    20]\n","2024-02-11 17:02:24,532 [INFO] loss: 0.0078    [8,    10]\n","2024-02-11 17:02:24,548 [INFO] loss: 0.0088    [8,    20]\n","2024-02-11 17:02:24,568 [INFO] loss: 0.0078    [9,    10]\n","2024-02-11 17:02:24,584 [INFO] loss: 0.0077    [9,    20]\n","2024-02-11 17:02:24,604 [INFO] loss: 0.0075    [10,    10]\n","2024-02-11 17:02:24,620 [INFO] loss: 0.0080    [10,    20]\n","2024-02-11 17:02:24,642 [INFO] loss: 0.0062    [11,    10]\n","2024-02-11 17:02:24,659 [INFO] loss: 0.0079    [11,    20]\n","2024-02-11 17:02:24,680 [INFO] loss: 0.0062    [12,    10]\n","2024-02-11 17:02:24,696 [INFO] loss: 0.0079    [12,    20]\n","2024-02-11 17:02:24,716 [INFO] loss: 0.0068    [13,    10]\n","2024-02-11 17:02:24,731 [INFO] loss: 0.0061    [13,    20]\n","2024-02-11 17:02:24,751 [INFO] loss: 0.0058    [14,    10]\n","2024-02-11 17:02:24,769 [INFO] loss: 0.0057    [14,    20]\n","2024-02-11 17:02:24,790 [INFO] loss: 0.0053    [15,    10]\n","2024-02-11 17:02:24,811 [INFO] loss: 0.0068    [15,    20]\n","2024-02-11 17:02:24,836 [INFO] loss: 0.0051    [16,    10]\n","2024-02-11 17:02:24,853 [INFO] loss: 0.0046    [16,    20]\n","2024-02-11 17:02:24,873 [INFO] loss: 0.0053    [17,    10]\n","2024-02-11 17:02:24,890 [INFO] loss: 0.0047    [17,    20]\n","2024-02-11 17:02:24,914 [INFO] loss: 0.0047    [18,    10]\n","2024-02-11 17:02:24,930 [INFO] loss: 0.0049    [18,    20]\n","2024-02-11 17:02:24,950 [INFO] loss: 0.0045    [19,    10]\n","2024-02-11 17:02:24,966 [INFO] loss: 0.0053    [19,    20]\n","2024-02-11 17:02:24,988 [INFO] loss: 0.0046    [20,    10]\n","2024-02-11 17:02:25,006 [INFO] loss: 0.0049    [20,    20]\n","2024-02-11 17:02:25,025 [INFO] loss: 0.0044    [21,    10]\n","2024-02-11 17:02:25,045 [INFO] loss: 0.0043    [21,    20]\n","2024-02-11 17:02:25,070 [INFO] loss: 0.0036    [22,    10]\n","2024-02-11 17:02:25,090 [INFO] loss: 0.0046    [22,    20]\n","2024-02-11 17:02:25,114 [INFO] loss: 0.0033    [23,    10]\n","2024-02-11 17:02:25,132 [INFO] loss: 0.0035    [23,    20]\n","2024-02-11 17:02:25,154 [INFO] loss: 0.0047    [24,    10]\n","2024-02-11 17:02:25,172 [INFO] loss: 0.0029    [24,    20]\n","2024-02-11 17:02:25,195 [INFO] loss: 0.0027    [25,    10]\n","2024-02-11 17:02:25,211 [INFO] loss: 0.0035    [25,    20]\n","2024-02-11 17:02:25,235 [INFO] loss: 0.0033    [26,    10]\n","2024-02-11 17:02:25,252 [INFO] loss: 0.0023    [26,    20]\n","2024-02-11 17:02:25,272 [INFO] loss: 0.0030    [27,    10]\n","2024-02-11 17:02:25,292 [INFO] loss: 0.0029    [27,    20]\n","2024-02-11 17:02:25,313 [INFO] loss: 0.0026    [28,    10]\n","2024-02-11 17:02:25,329 [INFO] loss: 0.0029    [28,    20]\n","2024-02-11 17:02:25,349 [INFO] loss: 0.0017    [29,    10]\n","2024-02-11 17:02:25,365 [INFO] loss: 0.0032    [29,    20]\n","2024-02-11 17:02:25,392 [INFO] loss: 0.0023    [30,    10]\n","2024-02-11 17:02:25,414 [INFO] loss: 0.0035    [30,    20]\n","2024-02-11 17:02:25,435 [INFO] loss: 0.0027    [31,    10]\n","2024-02-11 17:02:25,451 [INFO] loss: 0.0032    [31,    20]\n","2024-02-11 17:02:25,473 [INFO] loss: 0.0022    [32,    10]\n","2024-02-11 17:02:25,490 [INFO] loss: 0.0027    [32,    20]\n","2024-02-11 17:02:25,510 [INFO] loss: 0.0036    [33,    10]\n","2024-02-11 17:02:25,527 [INFO] loss: 0.0029    [33,    20]\n","2024-02-11 17:02:25,548 [INFO] loss: 0.0024    [34,    10]\n","2024-02-11 17:02:25,567 [INFO] loss: 0.0026    [34,    20]\n","2024-02-11 17:02:25,587 [INFO] loss: 0.0020    [35,    10]\n","2024-02-11 17:02:25,603 [INFO] loss: 0.0026    [35,    20]\n","2024-02-11 17:02:25,623 [INFO] loss: 0.0015    [36,    10]\n","2024-02-11 17:02:25,639 [INFO] loss: 0.0023    [36,    20]\n","2024-02-11 17:02:25,659 [INFO] loss: 0.0023    [37,    10]\n","2024-02-11 17:02:25,676 [INFO] loss: 0.0022    [37,    20]\n","2024-02-11 17:02:25,696 [INFO] loss: 0.0014    [38,    10]\n","2024-02-11 17:02:25,713 [INFO] loss: 0.0027    [38,    20]\n","2024-02-11 17:02:25,734 [INFO] loss: 0.0020    [39,    10]\n","2024-02-11 17:02:25,750 [INFO] loss: 0.0033    [39,    20]\n","2024-02-11 17:02:25,771 [INFO] loss: 0.0019    [40,    10]\n","2024-02-11 17:02:25,787 [INFO] loss: 0.0031    [40,    20]\n","2024-02-11 17:02:25,808 [INFO] loss: 0.0019    [41,    10]\n","2024-02-11 17:02:25,829 [INFO] loss: 0.0026    [41,    20]\n","2024-02-11 17:02:25,860 [INFO] loss: 0.0031    [42,    10]\n","2024-02-11 17:02:25,876 [INFO] loss: 0.0028    [42,    20]\n","2024-02-11 17:02:25,897 [INFO] loss: 0.0018    [43,    10]\n","2024-02-11 17:02:25,914 [INFO] loss: 0.0022    [43,    20]\n","2024-02-11 17:02:25,936 [INFO] loss: 0.0021    [44,    10]\n","2024-02-11 17:02:25,952 [INFO] loss: 0.0019    [44,    20]\n","2024-02-11 17:02:25,972 [INFO] loss: 0.0025    [45,    10]\n","2024-02-11 17:02:25,992 [INFO] loss: 0.0028    [45,    20]\n","2024-02-11 17:02:26,014 [INFO] loss: 0.0024    [46,    10]\n","2024-02-11 17:02:26,031 [INFO] loss: 0.0020    [46,    20]\n","2024-02-11 17:02:26,051 [INFO] loss: 0.0027    [47,    10]\n","2024-02-11 17:02:26,069 [INFO] loss: 0.0020    [47,    20]\n","2024-02-11 17:02:26,089 [INFO] loss: 0.0016    [48,    10]\n","2024-02-11 17:02:26,106 [INFO] loss: 0.0019    [48,    20]\n","2024-02-11 17:02:26,127 [INFO] loss: 0.0017    [49,    10]\n","2024-02-11 17:02:26,144 [INFO] loss: 0.0015    [49,    20]\n","2024-02-11 17:02:26,165 [INFO] loss: 0.0012    [50,    10]\n","2024-02-11 17:02:26,182 [INFO] loss: 0.0022    [50,    20]\n","2024-02-11 17:02:26,224 [INFO] Result on Train Data : {'AUC': 0.9998916727278354, 'ACC': 0.9902642559109874, 'F1 Score': 0.9902641805798161, 'AUPR': 0, 'Loss': 0.02807841806308083}\n","2024-02-11 17:02:26,225 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:26,228 [INFO] moving data and model to cpu\n","2024-02-11 17:02:26,249 [INFO] Result on Test Data : {'AUC': 0.8959790209790209, 'ACC': 0.8100558659217877, 'F1 Score': 0.8090486947791165, 'AUPR': 0, 'Loss': 0.8057260935505232}\n","2024-02-11 17:02:26,251 [INFO] Result of fold 4 : {'AUC': 0.8959790209790209, 'ACC': 0.8100558659217877, 'F1 Score': 0.8090486947791165, 'AUPR': 0, 'Loss': 0.8057260935505232}\n","2024-02-11 17:02:26,253 [INFO] ---- Fold 5 ----\n","2024-02-11 17:02:26,256 [INFO] Initializing SimplePytorchData with X shape : torch.Size([716, 64]) and y shape : torch.Size([716, 1])\n","2024-02-11 17:02:26,259 [INFO] Initializing SimplePytorchData with X shape : torch.Size([182, 64]) and y shape : torch.Size([182, 1])\n","2024-02-11 17:02:26,261 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 17:02:26,263 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 17:02:26,265 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 17:02:26,267 [INFO] moving data and model to cpu\n","2024-02-11 17:02:26,287 [INFO] loss: 0.0177    [1,    10]\n","2024-02-11 17:02:26,303 [INFO] loss: 0.0156    [1,    20]\n","2024-02-11 17:02:26,328 [INFO] loss: 0.0127    [2,    10]\n","2024-02-11 17:02:26,344 [INFO] loss: 0.0125    [2,    20]\n","2024-02-11 17:02:26,364 [INFO] loss: 0.0122    [3,    10]\n","2024-02-11 17:02:26,379 [INFO] loss: 0.0109    [3,    20]\n","2024-02-11 17:02:26,402 [INFO] loss: 0.0103    [4,    10]\n","2024-02-11 17:02:26,420 [INFO] loss: 0.0102    [4,    20]\n","2024-02-11 17:02:26,443 [INFO] loss: 0.0107    [5,    10]\n","2024-02-11 17:02:26,460 [INFO] loss: 0.0087    [5,    20]\n","2024-02-11 17:02:26,489 [INFO] loss: 0.0086    [6,    10]\n","2024-02-11 17:02:26,510 [INFO] loss: 0.0086    [6,    20]\n","2024-02-11 17:02:26,538 [INFO] loss: 0.0080    [7,    10]\n","2024-02-11 17:02:26,563 [INFO] loss: 0.0081    [7,    20]\n","2024-02-11 17:02:26,584 [INFO] loss: 0.0075    [8,    10]\n","2024-02-11 17:02:26,600 [INFO] loss: 0.0071    [8,    20]\n","2024-02-11 17:02:26,620 [INFO] loss: 0.0058    [9,    10]\n","2024-02-11 17:02:26,636 [INFO] loss: 0.0075    [9,    20]\n","2024-02-11 17:02:26,657 [INFO] loss: 0.0069    [10,    10]\n","2024-02-11 17:02:26,673 [INFO] loss: 0.0073    [10,    20]\n","2024-02-11 17:02:26,692 [INFO] loss: 0.0070    [11,    10]\n","2024-02-11 17:02:26,708 [INFO] loss: 0.0069    [11,    20]\n","2024-02-11 17:02:26,727 [INFO] loss: 0.0054    [12,    10]\n","2024-02-11 17:02:26,743 [INFO] loss: 0.0063    [12,    20]\n","2024-02-11 17:02:26,763 [INFO] loss: 0.0058    [13,    10]\n","2024-02-11 17:02:26,778 [INFO] loss: 0.0057    [13,    20]\n","2024-02-11 17:02:26,798 [INFO] loss: 0.0061    [14,    10]\n","2024-02-11 17:02:26,816 [INFO] loss: 0.0053    [14,    20]\n","2024-02-11 17:02:26,836 [INFO] loss: 0.0051    [15,    10]\n","2024-02-11 17:02:26,852 [INFO] loss: 0.0059    [15,    20]\n","2024-02-11 17:02:26,880 [INFO] loss: 0.0045    [16,    10]\n","2024-02-11 17:02:26,898 [INFO] loss: 0.0050    [16,    20]\n","2024-02-11 17:02:26,918 [INFO] loss: 0.0052    [17,    10]\n","2024-02-11 17:02:26,934 [INFO] loss: 0.0051    [17,    20]\n","2024-02-11 17:02:26,954 [INFO] loss: 0.0046    [18,    10]\n","2024-02-11 17:02:26,971 [INFO] loss: 0.0047    [18,    20]\n","2024-02-11 17:02:26,991 [INFO] loss: 0.0035    [19,    10]\n","2024-02-11 17:02:27,007 [INFO] loss: 0.0046    [19,    20]\n","2024-02-11 17:02:27,028 [INFO] loss: 0.0035    [20,    10]\n","2024-02-11 17:02:27,049 [INFO] loss: 0.0040    [20,    20]\n","2024-02-11 17:02:27,069 [INFO] loss: 0.0036    [21,    10]\n","2024-02-11 17:02:27,086 [INFO] loss: 0.0036    [21,    20]\n","2024-02-11 17:02:27,109 [INFO] loss: 0.0029    [22,    10]\n","2024-02-11 17:02:27,126 [INFO] loss: 0.0028    [22,    20]\n","2024-02-11 17:02:27,146 [INFO] loss: 0.0031    [23,    10]\n","2024-02-11 17:02:27,162 [INFO] loss: 0.0032    [23,    20]\n","2024-02-11 17:02:27,182 [INFO] loss: 0.0033    [24,    10]\n","2024-02-11 17:02:27,198 [INFO] loss: 0.0030    [24,    20]\n","2024-02-11 17:02:27,220 [INFO] loss: 0.0031    [25,    10]\n","2024-02-11 17:02:27,239 [INFO] loss: 0.0020    [25,    20]\n","2024-02-11 17:02:27,262 [INFO] loss: 0.0024    [26,    10]\n","2024-02-11 17:02:27,279 [INFO] loss: 0.0036    [26,    20]\n","2024-02-11 17:02:27,302 [INFO] loss: 0.0023    [27,    10]\n","2024-02-11 17:02:27,318 [INFO] loss: 0.0032    [27,    20]\n","2024-02-11 17:02:27,339 [INFO] loss: 0.0036    [28,    10]\n","2024-02-11 17:02:27,358 [INFO] loss: 0.0029    [28,    20]\n","2024-02-11 17:02:27,378 [INFO] loss: 0.0030    [29,    10]\n","2024-02-11 17:02:27,394 [INFO] loss: 0.0028    [29,    20]\n","2024-02-11 17:02:27,415 [INFO] loss: 0.0032    [30,    10]\n","2024-02-11 17:02:27,435 [INFO] loss: 0.0026    [30,    20]\n","2024-02-11 17:02:27,456 [INFO] loss: 0.0025    [31,    10]\n","2024-02-11 17:02:27,472 [INFO] loss: 0.0025    [31,    20]\n","2024-02-11 17:02:27,494 [INFO] loss: 0.0015    [32,    10]\n","2024-02-11 17:02:27,510 [INFO] loss: 0.0020    [32,    20]\n","2024-02-11 17:02:27,530 [INFO] loss: 0.0025    [33,    10]\n","2024-02-11 17:02:27,549 [INFO] loss: 0.0025    [33,    20]\n","2024-02-11 17:02:27,570 [INFO] loss: 0.0021    [34,    10]\n","2024-02-11 17:02:27,587 [INFO] loss: 0.0018    [34,    20]\n","2024-02-11 17:02:27,608 [INFO] loss: 0.0023    [35,    10]\n","2024-02-11 17:02:27,624 [INFO] loss: 0.0017    [35,    20]\n","2024-02-11 17:02:27,645 [INFO] loss: 0.0015    [36,    10]\n","2024-02-11 17:02:27,665 [INFO] loss: 0.0020    [36,    20]\n","2024-02-11 17:02:27,686 [INFO] loss: 0.0027    [37,    10]\n","2024-02-11 17:02:27,703 [INFO] loss: 0.0014    [37,    20]\n","2024-02-11 17:02:27,723 [INFO] loss: 0.0012    [38,    10]\n","2024-02-11 17:02:27,740 [INFO] loss: 0.0019    [38,    20]\n","2024-02-11 17:02:27,761 [INFO] loss: 0.0031    [39,    10]\n","2024-02-11 17:02:27,778 [INFO] loss: 0.0030    [39,    20]\n","2024-02-11 17:02:27,799 [INFO] loss: 0.0018    [40,    10]\n","2024-02-11 17:02:27,816 [INFO] loss: 0.0022    [40,    20]\n","2024-02-11 17:02:27,837 [INFO] loss: 0.0017    [41,    10]\n","2024-02-11 17:02:27,855 [INFO] loss: 0.0024    [41,    20]\n","2024-02-11 17:02:27,876 [INFO] loss: 0.0013    [42,    10]\n","2024-02-11 17:02:27,899 [INFO] loss: 0.0014    [42,    20]\n","2024-02-11 17:02:27,926 [INFO] loss: 0.0013    [43,    10]\n","2024-02-11 17:02:27,942 [INFO] loss: 0.0016    [43,    20]\n","2024-02-11 17:02:27,965 [INFO] loss: 0.0011    [44,    10]\n","2024-02-11 17:02:27,982 [INFO] loss: 0.0011    [44,    20]\n","2024-02-11 17:02:28,004 [INFO] loss: 0.0014    [45,    10]\n","2024-02-11 17:02:28,021 [INFO] loss: 0.0013    [45,    20]\n","2024-02-11 17:02:28,043 [INFO] loss: 0.0013    [46,    10]\n","2024-02-11 17:02:28,064 [INFO] loss: 0.0015    [46,    20]\n","2024-02-11 17:02:28,084 [INFO] loss: 0.0017    [47,    10]\n","2024-02-11 17:02:28,102 [INFO] loss: 0.0024    [47,    20]\n","2024-02-11 17:02:28,122 [INFO] loss: 0.0027    [48,    10]\n","2024-02-11 17:02:28,139 [INFO] loss: 0.0015    [48,    20]\n","2024-02-11 17:02:28,159 [INFO] loss: 0.0019    [49,    10]\n","2024-02-11 17:02:28,175 [INFO] loss: 0.0011    [49,    20]\n","2024-02-11 17:02:28,195 [INFO] loss: 0.0013    [50,    10]\n","2024-02-11 17:02:28,211 [INFO] loss: 0.0013    [50,    20]\n","2024-02-11 17:02:28,253 [INFO] Result on Train Data : {'AUC': 0.9999843906093906, 'ACC': 0.9986033519553073, 'F1 Score': 0.9986030222326281, 'AUPR': 0, 'Loss': 0.016125619269745504}\n","2024-02-11 17:02:28,255 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 17:02:28,257 [INFO] moving data and model to cpu\n","2024-02-11 17:02:28,278 [INFO] Result on Test Data : {'AUC': 0.904426925409339, 'ACC': 0.8406593406593407, 'F1 Score': 0.839256890513172, 'AUPR': 0, 'Loss': 1.1888436476389568}\n","2024-02-11 17:02:28,280 [INFO] Result of fold 5 : {'AUC': 0.904426925409339, 'ACC': 0.8406593406593407, 'F1 Score': 0.839256890513172, 'AUPR': 0, 'Loss': 1.1888436476389568}\n","2024-02-11 17:02:28,282 [INFO] 5-fold result: avg_auc: 0.8820107382571971, avg_acc: 0.7938301921542146, avg_f1: 0.7927998427281563, avg_aupr: 0.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<base.evaluation.Result at 0x7c21126899f0>"]},"metadata":{},"execution_count":20}],"source":["trainer = SimpleTrainer()\n","tester = SimpleTester()\n","factory = SimpleMDAClassifierFactory(simple_classifier_config)\n","spliter = SimplePytorchDataTrainTestSplit(data)\n","cross_validation(k=5, data_size=data.X.shape[0], train_test_spliter=spliter, model_factory=factory,\n","                    trainer=trainer, tester=tester, config=classifier_optimizer_config)"],"id":"sfI286uv6o-e"},{"cell_type":"code","source":[],"metadata":{"id":"5cpQ0kPkjuHb","executionInfo":{"status":"ok","timestamp":1707670948641,"user_tz":-210,"elapsed":3,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":20,"outputs":[],"id":"5cpQ0kPkjuHb"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbSkLzXRVkG1","executionInfo":{"status":"ok","timestamp":1707658123140,"user_tz":-210,"elapsed":19348,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"a2f89f5c-60c4-48e6-d38f-eb79f8301706"},"id":"AbSkLzXRVkG1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"initial_id","metadata":{"collapsed":true,"id":"initial_id","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707658123140,"user_tz":-210,"elapsed":7,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"54f0960b-5655-4497-b1f4-449d6e2c2751"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Academic/Topics/AI/Machine\\ Learning\\ Dr.\\ Montazeri/Project/ml_mda"],"metadata":{"id":"uDVTuQuDVXDp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707658123765,"user_tz":-210,"elapsed":628,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"0e3bbb4f-4762-4542-b939-e44e506f0051"},"id":"uDVTuQuDVXDp","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Academic/Topics/AI/Machine Learning Dr. Montazeri/Project/ml_mda\n"]}]},{"cell_type":"code","source":["!pip install torch_geometric"],"metadata":{"id":"RDY-Ibk-EuwN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707658134021,"user_tz":-210,"elapsed":10260,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"1e88c512-f11a-43d8-9e6a-ba4aa900dab5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n"]}],"id":"RDY-Ibk-EuwN"},{"cell_type":"code","source":["!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html"],"metadata":{"id":"8Vot3XZDEzNX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707658148792,"user_tz":-210,"elapsed":14777,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"e4ae1ee3-dc75-4ca2-a318-169335515514"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","Collecting pyg_lib\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/pyg_lib-0.4.0%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_scatter\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_sparse\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_cluster\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch_spline_conv\n","  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (932 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.1/932.1 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n","Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n","Successfully installed pyg_lib-0.4.0+pt21cu121 torch_cluster-1.6.3+pt21cu121 torch_scatter-2.1.2+pt21cu121 torch_sparse-0.6.18+pt21cu121 torch_spline_conv-1.2.2+pt21cu121\n"]}],"id":"8Vot3XZDEzNX"},{"cell_type":"markdown","source":["# Requirements"],"metadata":{"id":"CP5slJxnWMG-"},"id":"CP5slJxnWMG-"},{"cell_type":"code","source":["import torch\n","\n","from torch.optim import Adam\n","from torch_geometric.nn import ComplEx, DistMult, RotatE, TransE\n","from torch_geometric.data import Data\n","\n","from base import OptimizerConfig, cross_validation\n","from base import SimplePytorchData, SimplePytorchDataTrainTestSplit\n","from base import SimpleTrainer, SimpleTester\n","from src.config import SimpleClassifierConfig, GraphAutoEncoderConfig, KGEConfig\n","from src.features import get_relations, get_entities, get_associations, get_homogeneous_graph, get_kge_pair_embedd_for_training_data\n","from src.models import SimpleMDAClassifier, SimpleMDAClassifierFactory\n","from src.utils import train_test_sampler, prj_logger\n","from torch_geometric.nn import GCNConv"],"metadata":{"id":"1Tz_6Gnq191K","executionInfo":{"status":"ok","timestamp":1707658169339,"user_tz":-210,"elapsed":20552,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"1Tz_6Gnq191K","execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"5V-nDmj61M_3","executionInfo":{"status":"ok","timestamp":1707658169339,"user_tz":-210,"elapsed":24,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"5V-nDmj61M_3","execution_count":7,"outputs":[]},{"cell_type":"code","source":["import logging\n","import sys\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    handlers=[\n","        logging.StreamHandler(stream=sys.stdout)\n","    ],\n","    force=True\n",")"],"metadata":{"id":"v4fhFqHr-UQI","executionInfo":{"status":"ok","timestamp":1707658169339,"user_tz":-210,"elapsed":23,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"v4fhFqHr-UQI","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# TransE"],"metadata":{"id":"jQ9m4sXXMrP_"},"id":"jQ9m4sXXMrP_"},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"mze45lXFT9mw"},"id":"mze45lXFT9mw"},{"cell_type":"code","source":["kge_optimizer_config = OptimizerConfig()\n","kge_optimizer_config.optimizer = torch.optim.Adam\n","kge_optimizer_config.lr = 0.01\n","kge_optimizer_config.batch_size = 1000\n","kge_optimizer_config.n_epoch = 100\n","kge_optimizer_config.exp_name = \"Optimizer for Graph Auto Encoder\"\n","kge_optimizer_config.device = device\n","kge_optimizer_config.report_size = device"],"metadata":{"id":"r4zjaDCMMsJC","executionInfo":{"status":"ok","timestamp":1707658169339,"user_tz":-210,"elapsed":24,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"r4zjaDCMMsJC","execution_count":9,"outputs":[]},{"cell_type":"code","source":["kge_model_config = KGEConfig()\n","kge_model_config.kge = TransE\n","kge_model_config.hidden_channels = 32"],"metadata":{"id":"KEPyGw8LNBFv","executionInfo":{"status":"ok","timestamp":1707658169340,"user_tz":-210,"elapsed":24,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"id":"KEPyGw8LNBFv","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Embedding"],"metadata":{"id":"LKtvRPGwT_NG"},"id":"LKtvRPGwT_NG"},{"cell_type":"code","source":["md_embed = get_kge_pair_embedd_for_training_data(kge_model_config, kge_optimizer_config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2Qz4EGMOrrk","executionInfo":{"status":"ok","timestamp":1707660573638,"user_tz":-210,"elapsed":2404321,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"04920100-b2a3-4755-f703-e0497d931dce"},"id":"r2Qz4EGMOrrk","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 13:29:29,359 [INFO] Calling get_node2vec_pair_embedd on cpu device ...\n","2024-02-11 13:29:29,361 [INFO] Calling get_homogeneous_graph\n","2024-02-11 13:29:31,087 [INFO] homogeneous data : Data(x=[66911, 1], edge_index=[2, 633662])\n","2024-02-11 13:29:31,097 [INFO] Calling get_kge_embedd on cpu device ...\n","2024-02-11 13:29:31,099 [INFO] Calling get_knowledge_graph\n","2024-02-11 13:29:32,360 [INFO] knowledge graph data : Data(num_nodes=66911, edge_index=[2, 633662], edge_type=[633662])\n","2024-02-11 13:29:32,392 [INFO] Setting num relations and num nodes for kge config to 39 and 66911\n","2024-02-11 13:29:32,395 [INFO] Creating KGE model ...\n","2024-02-11 13:29:32,398 [INFO] Initialing MDATransE with model_config {'model_name': None}\n","2024-02-11 13:29:32,471 [INFO] Training KGE ...\n","2024-02-11 13:29:32,473 [INFO] Running KGETrainer with Optimizer for Graph Auto Encoder\n","2024-02-11 13:29:32,475 [INFO] Creating <class 'torch.optim.adam.Adam'> with lr : 0.01\n","2024-02-11 13:29:32,477 [INFO] moving model to cpu\n","2024-02-11 13:30:01,020 [INFO] Epoch: 001, Loss: 0.8699\n","2024-02-11 13:30:24,702 [INFO] Epoch: 002, Loss: 0.6295\n","2024-02-11 13:30:48,595 [INFO] Epoch: 003, Loss: 0.4818\n","2024-02-11 13:31:12,682 [INFO] Epoch: 004, Loss: 0.4073\n","2024-02-11 13:31:36,780 [INFO] Epoch: 005, Loss: 0.3689\n","2024-02-11 13:32:00,899 [INFO] Epoch: 006, Loss: 0.3454\n","2024-02-11 13:32:24,505 [INFO] Epoch: 007, Loss: 0.3318\n","2024-02-11 13:32:47,905 [INFO] Epoch: 008, Loss: 0.3204\n","2024-02-11 13:33:12,209 [INFO] Epoch: 009, Loss: 0.3099\n","2024-02-11 13:33:32,545 [INFO] Epoch: 010, Loss: 0.3023\n","2024-02-11 13:33:55,689 [INFO] Epoch: 011, Loss: 0.2956\n","2024-02-11 13:34:20,097 [INFO] Epoch: 012, Loss: 0.2881\n","2024-02-11 13:34:44,311 [INFO] Epoch: 013, Loss: 0.2816\n","2024-02-11 13:35:08,942 [INFO] Epoch: 014, Loss: 0.2754\n","2024-02-11 13:35:33,174 [INFO] Epoch: 015, Loss: 0.2697\n","2024-02-11 13:35:57,257 [INFO] Epoch: 016, Loss: 0.2648\n","2024-02-11 13:36:21,090 [INFO] Epoch: 017, Loss: 0.2599\n","2024-02-11 13:36:45,086 [INFO] Epoch: 018, Loss: 0.2559\n","2024-02-11 13:37:05,151 [INFO] Epoch: 019, Loss: 0.2524\n","2024-02-11 13:37:29,760 [INFO] Epoch: 020, Loss: 0.2493\n","2024-02-11 13:37:53,769 [INFO] Epoch: 021, Loss: 0.2451\n","2024-02-11 13:38:18,513 [INFO] Epoch: 022, Loss: 0.2433\n","2024-02-11 13:38:42,386 [INFO] Epoch: 023, Loss: 0.2398\n","2024-02-11 13:39:06,284 [INFO] Epoch: 024, Loss: 0.2377\n","2024-02-11 13:39:30,358 [INFO] Epoch: 025, Loss: 0.2354\n","2024-02-11 13:39:54,810 [INFO] Epoch: 026, Loss: 0.2329\n","2024-02-11 13:40:19,685 [INFO] Epoch: 027, Loss: 0.2308\n","2024-02-11 13:40:43,205 [INFO] Epoch: 028, Loss: 0.2293\n","2024-02-11 13:41:06,741 [INFO] Epoch: 029, Loss: 0.2273\n","2024-02-11 13:41:30,961 [INFO] Epoch: 030, Loss: 0.2257\n","2024-02-11 13:41:55,272 [INFO] Epoch: 031, Loss: 0.2236\n","2024-02-11 13:42:15,679 [INFO] Epoch: 032, Loss: 0.2229\n","2024-02-11 13:42:39,531 [INFO] Epoch: 033, Loss: 0.2206\n","2024-02-11 13:43:00,427 [INFO] Epoch: 034, Loss: 0.2195\n","2024-02-11 13:43:25,237 [INFO] Epoch: 035, Loss: 0.2189\n","2024-02-11 13:43:49,041 [INFO] Epoch: 036, Loss: 0.2164\n","2024-02-11 13:44:09,290 [INFO] Epoch: 037, Loss: 0.2162\n","2024-02-11 13:44:33,676 [INFO] Epoch: 038, Loss: 0.2145\n","2024-02-11 13:44:58,474 [INFO] Epoch: 039, Loss: 0.2135\n","2024-02-11 13:45:22,479 [INFO] Epoch: 040, Loss: 0.2130\n","2024-02-11 13:45:47,118 [INFO] Epoch: 041, Loss: 0.2111\n","2024-02-11 13:46:07,612 [INFO] Epoch: 042, Loss: 0.2101\n","2024-02-11 13:46:31,799 [INFO] Epoch: 043, Loss: 0.2096\n","2024-02-11 13:46:52,782 [INFO] Epoch: 044, Loss: 0.2080\n","2024-02-11 13:47:16,821 [INFO] Epoch: 045, Loss: 0.2074\n","2024-02-11 13:47:41,111 [INFO] Epoch: 046, Loss: 0.2075\n","2024-02-11 13:48:04,794 [INFO] Epoch: 047, Loss: 0.2060\n","2024-02-11 13:48:28,614 [INFO] Epoch: 048, Loss: 0.2054\n","2024-02-11 13:48:53,150 [INFO] Epoch: 049, Loss: 0.2049\n","2024-02-11 13:49:18,000 [INFO] Epoch: 050, Loss: 0.2044\n","2024-02-11 13:49:42,172 [INFO] Epoch: 051, Loss: 0.2032\n","2024-02-11 13:50:05,607 [INFO] Epoch: 052, Loss: 0.2018\n","2024-02-11 13:50:29,980 [INFO] Epoch: 053, Loss: 0.2016\n","2024-02-11 13:50:54,385 [INFO] Epoch: 054, Loss: 0.2005\n","2024-02-11 13:51:19,122 [INFO] Epoch: 055, Loss: 0.2005\n","2024-02-11 13:51:39,145 [INFO] Epoch: 056, Loss: 0.1998\n","2024-02-11 13:52:03,469 [INFO] Epoch: 057, Loss: 0.1996\n","2024-02-11 13:52:28,006 [INFO] Epoch: 058, Loss: 0.1994\n","2024-02-11 13:52:47,950 [INFO] Epoch: 059, Loss: 0.1986\n","2024-02-11 13:53:12,367 [INFO] Epoch: 060, Loss: 0.1975\n","2024-02-11 13:53:37,211 [INFO] Epoch: 061, Loss: 0.1972\n","2024-02-11 13:54:01,586 [INFO] Epoch: 062, Loss: 0.1965\n","2024-02-11 13:54:26,062 [INFO] Epoch: 063, Loss: 0.1961\n","2024-02-11 13:54:49,673 [INFO] Epoch: 064, Loss: 0.1960\n","2024-02-11 13:55:13,608 [INFO] Epoch: 065, Loss: 0.1950\n","2024-02-11 13:55:37,493 [INFO] Epoch: 066, Loss: 0.1947\n","2024-02-11 13:56:01,640 [INFO] Epoch: 067, Loss: 0.1934\n","2024-02-11 13:56:25,982 [INFO] Epoch: 068, Loss: 0.1934\n","2024-02-11 13:56:49,878 [INFO] Epoch: 069, Loss: 0.1934\n","2024-02-11 13:57:14,017 [INFO] Epoch: 070, Loss: 0.1923\n","2024-02-11 13:57:38,657 [INFO] Epoch: 071, Loss: 0.1922\n","2024-02-11 13:58:02,941 [INFO] Epoch: 072, Loss: 0.1916\n","2024-02-11 13:58:27,658 [INFO] Epoch: 073, Loss: 0.1910\n","2024-02-11 13:58:51,962 [INFO] Epoch: 074, Loss: 0.1916\n","2024-02-11 13:59:16,248 [INFO] Epoch: 075, Loss: 0.1913\n","2024-02-11 13:59:40,850 [INFO] Epoch: 076, Loss: 0.1902\n","2024-02-11 14:00:05,903 [INFO] Epoch: 077, Loss: 0.1901\n","2024-02-11 14:00:30,933 [INFO] Epoch: 078, Loss: 0.1901\n","2024-02-11 14:00:55,601 [INFO] Epoch: 079, Loss: 0.1888\n","2024-02-11 14:01:20,359 [INFO] Epoch: 080, Loss: 0.1888\n","2024-02-11 14:01:44,893 [INFO] Epoch: 081, Loss: 0.1886\n","2024-02-11 14:02:10,204 [INFO] Epoch: 082, Loss: 0.1882\n","2024-02-11 14:02:34,891 [INFO] Epoch: 083, Loss: 0.1879\n","2024-02-11 14:02:59,694 [INFO] Epoch: 084, Loss: 0.1873\n","2024-02-11 14:03:21,116 [INFO] Epoch: 085, Loss: 0.1868\n","2024-02-11 14:03:46,267 [INFO] Epoch: 086, Loss: 0.1867\n","2024-02-11 14:04:10,904 [INFO] Epoch: 087, Loss: 0.1868\n","2024-02-11 14:04:35,370 [INFO] Epoch: 088, Loss: 0.1860\n","2024-02-11 14:04:59,924 [INFO] Epoch: 089, Loss: 0.1864\n","2024-02-11 14:05:24,345 [INFO] Epoch: 090, Loss: 0.1852\n","2024-02-11 14:05:48,624 [INFO] Epoch: 091, Loss: 0.1848\n","2024-02-11 14:06:13,881 [INFO] Epoch: 092, Loss: 0.1847\n","2024-02-11 14:06:39,057 [INFO] Epoch: 093, Loss: 0.1847\n","2024-02-11 14:07:03,894 [INFO] Epoch: 094, Loss: 0.1839\n","2024-02-11 14:07:28,860 [INFO] Epoch: 095, Loss: 0.1838\n","2024-02-11 14:07:54,059 [INFO] Epoch: 096, Loss: 0.1837\n","2024-02-11 14:08:18,243 [INFO] Epoch: 097, Loss: 0.1829\n","2024-02-11 14:08:42,794 [INFO] Epoch: 098, Loss: 0.1832\n","2024-02-11 14:09:07,617 [INFO] Epoch: 099, Loss: 0.1824\n","2024-02-11 14:09:33,170 [INFO] Epoch: 100, Loss: 0.1821\n","2024-02-11 14:09:33,171 [INFO] Result on Train Data : {'AUC': 0, 'ACC': 0, 'F1 Score': 0, 'AUPR': 0, 'Loss': 0.1821357051620709}\n","2024-02-11 14:09:33,174 [INFO] loss of KGE model : 0.1821357051620709\n","2024-02-11 14:09:33,195 [INFO] node embedding shape : torch.Size([66911, 32])\n","2024-02-11 14:09:33,199 [INFO] disease embedding shape : torch.Size([898, 32])\n","2024-02-11 14:09:33,201 [INFO] microbe embedding shape : torch.Size([898, 32])\n","2024-02-11 14:09:33,204 [INFO] microbe disease combination embedding shape : torch.Size([898, 64])\n"]}]},{"cell_type":"markdown","source":["# Classification"],"metadata":{"id":"T_hIMihJMts8"},"id":"T_hIMihJMts8"},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"ocxVXIz1MqLJ"},"id":"ocxVXIz1MqLJ"},{"cell_type":"code","source":["associations = get_associations()\n","y = torch.tensor(associations['increased'].tolist(), dtype=torch.float32).reshape(-1, 1).to(device)"],"metadata":{"id":"jEfB8KA7gPx2","executionInfo":{"status":"ok","timestamp":1707660573638,"user_tz":-210,"elapsed":32,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":12,"outputs":[],"id":"jEfB8KA7gPx2"},{"cell_type":"code","source":["# Train Test Split\n","train_indices, test_indices = train_test_sampler(y.shape[0], 0.7)\n","\n","data = SimplePytorchData(md_embed, y)\n","train_data = SimplePytorchData(md_embed[train_indices], y[train_indices])\n","test_data = SimplePytorchData(md_embed[test_indices], y[test_indices])"],"metadata":{"id":"DNdQlgzMMtHN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660573639,"user_tz":-210,"elapsed":13,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"170970e3-b107-4c79-91d8-85707944ae90"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 14:09:33,242 [INFO] Initializing SimplePytorchData with X shape : torch.Size([898, 64]) and y shape : torch.Size([898, 1])\n","2024-02-11 14:09:33,246 [INFO] Initializing SimplePytorchData with X shape : torch.Size([628, 64]) and y shape : torch.Size([628, 1])\n","2024-02-11 14:09:33,248 [INFO] Initializing SimplePytorchData with X shape : torch.Size([270, 64]) and y shape : torch.Size([270, 1])\n"]}],"id":"DNdQlgzMMtHN"},{"cell_type":"markdown","source":["## Classifier"],"metadata":{"id":"ye_6wl6nxmhs"},"id":"ye_6wl6nxmhs"},{"cell_type":"code","source":["simple_classifier_config = SimpleClassifierConfig()\n","simple_classifier_config.model_name = \"simple classifier\"\n","simple_classifier_config.input_dim = md_embed.shape[1]\n","simple_classifier_config.hidden_dim = 32\n","simple_classifier_config.output_dim = 1\n","simple_classifier_config.num_layers = 2\n","simple_classifier_config.dropout = 0.1"],"metadata":{"id":"BFTQsCl8M9bv","executionInfo":{"status":"ok","timestamp":1707660573639,"user_tz":-210,"elapsed":9,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":14,"outputs":[],"id":"BFTQsCl8M9bv"},{"cell_type":"code","source":["mda_classifier = SimpleMDAClassifier(simple_classifier_config)"],"metadata":{"id":"1ciyBQ4QM_0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660573639,"user_tz":-210,"elapsed":8,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"e1d20d8b-1953-4c21-d131-5224e76af9da"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 14:09:33,271 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:33,274 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n"]}],"id":"1ciyBQ4QM_0U"},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{"id":"s_5cdKvOx4q5"},"id":"s_5cdKvOx4q5"},{"cell_type":"code","source":["classifier_optimizer_config = OptimizerConfig()\n","classifier_optimizer_config.optimizer = torch.optim.Adam\n","classifier_optimizer_config.criterion = torch.nn.BCEWithLogitsLoss()\n","classifier_optimizer_config.lr = 0.01\n","classifier_optimizer_config.batch_size = 32\n","classifier_optimizer_config.n_epoch = 50\n","classifier_optimizer_config.exp_name = \"adam optimizer\"\n","classifier_optimizer_config.save = False\n","classifier_optimizer_config.save_path = None\n","classifier_optimizer_config.device = device\n","classifier_optimizer_config.report_size = 10  # batch to report ratio\n","classifier_optimizer_config.threshold = 0.5"],"metadata":{"id":"3D6yhiPpNEc8","executionInfo":{"status":"ok","timestamp":1707660573639,"user_tz":-210,"elapsed":7,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":16,"outputs":[],"id":"3D6yhiPpNEc8"},{"cell_type":"markdown","source":["## Train Test Approach"],"metadata":{"id":"4iI5bMmJNQV3"},"id":"4iI5bMmJNQV3"},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"h24KnmDZNAgD"},"id":"h24KnmDZNAgD"},{"cell_type":"code","source":["train_result = SimpleTrainer().train(model=mda_classifier,\n","                                     data=train_data,\n","                                     config=classifier_optimizer_config)"],"metadata":{"id":"OqKrF7HmNFx1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660575609,"user_tz":-210,"elapsed":1977,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"354fa0c8-85dc-4059-bb3e-09e1dc8158cc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 14:09:33,296 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:33,299 [INFO] moving data and model to cpu\n","2024-02-11 14:09:33,362 [INFO] loss: 0.0219    [1,    10]\n","2024-02-11 14:09:33,382 [INFO] loss: 0.0177    [1,    20]\n","2024-02-11 14:09:33,403 [INFO] loss: 0.0176    [2,    10]\n","2024-02-11 14:09:33,421 [INFO] loss: 0.0157    [2,    20]\n","2024-02-11 14:09:33,446 [INFO] loss: 0.0161    [3,    10]\n","2024-02-11 14:09:33,470 [INFO] loss: 0.0140    [3,    20]\n","2024-02-11 14:09:33,493 [INFO] loss: 0.0139    [4,    10]\n","2024-02-11 14:09:33,517 [INFO] loss: 0.0140    [4,    20]\n","2024-02-11 14:09:33,547 [INFO] loss: 0.0120    [5,    10]\n","2024-02-11 14:09:33,566 [INFO] loss: 0.0139    [5,    20]\n","2024-02-11 14:09:33,588 [INFO] loss: 0.0133    [6,    10]\n","2024-02-11 14:09:33,609 [INFO] loss: 0.0132    [6,    20]\n","2024-02-11 14:09:33,631 [INFO] loss: 0.0123    [7,    10]\n","2024-02-11 14:09:33,651 [INFO] loss: 0.0124    [7,    20]\n","2024-02-11 14:09:33,671 [INFO] loss: 0.0107    [8,    10]\n","2024-02-11 14:09:33,690 [INFO] loss: 0.0116    [8,    20]\n","2024-02-11 14:09:33,709 [INFO] loss: 0.0127    [9,    10]\n","2024-02-11 14:09:33,728 [INFO] loss: 0.0124    [9,    20]\n","2024-02-11 14:09:33,747 [INFO] loss: 0.0119    [10,    10]\n","2024-02-11 14:09:33,768 [INFO] loss: 0.0116    [10,    20]\n","2024-02-11 14:09:33,787 [INFO] loss: 0.0107    [11,    10]\n","2024-02-11 14:09:33,806 [INFO] loss: 0.0105    [11,    20]\n","2024-02-11 14:09:33,825 [INFO] loss: 0.0100    [12,    10]\n","2024-02-11 14:09:33,846 [INFO] loss: 0.0107    [12,    20]\n","2024-02-11 14:09:33,865 [INFO] loss: 0.0083    [13,    10]\n","2024-02-11 14:09:33,886 [INFO] loss: 0.0119    [13,    20]\n","2024-02-11 14:09:33,907 [INFO] loss: 0.0105    [14,    10]\n","2024-02-11 14:09:33,928 [INFO] loss: 0.0103    [14,    20]\n","2024-02-11 14:09:33,951 [INFO] loss: 0.0105    [15,    10]\n","2024-02-11 14:09:33,973 [INFO] loss: 0.0093    [15,    20]\n","2024-02-11 14:09:33,992 [INFO] loss: 0.0093    [16,    10]\n","2024-02-11 14:09:34,011 [INFO] loss: 0.0090    [16,    20]\n","2024-02-11 14:09:34,031 [INFO] loss: 0.0088    [17,    10]\n","2024-02-11 14:09:34,051 [INFO] loss: 0.0101    [17,    20]\n","2024-02-11 14:09:34,071 [INFO] loss: 0.0081    [18,    10]\n","2024-02-11 14:09:34,097 [INFO] loss: 0.0090    [18,    20]\n","2024-02-11 14:09:34,121 [INFO] loss: 0.0077    [19,    10]\n","2024-02-11 14:09:34,145 [INFO] loss: 0.0085    [19,    20]\n","2024-02-11 14:09:34,167 [INFO] loss: 0.0081    [20,    10]\n","2024-02-11 14:09:34,186 [INFO] loss: 0.0098    [20,    20]\n","2024-02-11 14:09:34,205 [INFO] loss: 0.0084    [21,    10]\n","2024-02-11 14:09:34,224 [INFO] loss: 0.0096    [21,    20]\n","2024-02-11 14:09:34,243 [INFO] loss: 0.0078    [22,    10]\n","2024-02-11 14:09:34,263 [INFO] loss: 0.0083    [22,    20]\n","2024-02-11 14:09:34,294 [INFO] loss: 0.0063    [23,    10]\n","2024-02-11 14:09:34,312 [INFO] loss: 0.0085    [23,    20]\n","2024-02-11 14:09:34,332 [INFO] loss: 0.0076    [24,    10]\n","2024-02-11 14:09:34,359 [INFO] loss: 0.0076    [24,    20]\n","2024-02-11 14:09:34,378 [INFO] loss: 0.0066    [25,    10]\n","2024-02-11 14:09:34,395 [INFO] loss: 0.0074    [25,    20]\n","2024-02-11 14:09:34,415 [INFO] loss: 0.0070    [26,    10]\n","2024-02-11 14:09:34,433 [INFO] loss: 0.0085    [26,    20]\n","2024-02-11 14:09:34,457 [INFO] loss: 0.0062    [27,    10]\n","2024-02-11 14:09:34,474 [INFO] loss: 0.0087    [27,    20]\n","2024-02-11 14:09:34,496 [INFO] loss: 0.0062    [28,    10]\n","2024-02-11 14:09:34,515 [INFO] loss: 0.0066    [28,    20]\n","2024-02-11 14:09:34,535 [INFO] loss: 0.0065    [29,    10]\n","2024-02-11 14:09:34,556 [INFO] loss: 0.0068    [29,    20]\n","2024-02-11 14:09:34,576 [INFO] loss: 0.0059    [30,    10]\n","2024-02-11 14:09:34,595 [INFO] loss: 0.0074    [30,    20]\n","2024-02-11 14:09:34,616 [INFO] loss: 0.0067    [31,    10]\n","2024-02-11 14:09:34,636 [INFO] loss: 0.0065    [31,    20]\n","2024-02-11 14:09:34,654 [INFO] loss: 0.0073    [32,    10]\n","2024-02-11 14:09:34,675 [INFO] loss: 0.0065    [32,    20]\n","2024-02-11 14:09:34,694 [INFO] loss: 0.0069    [33,    10]\n","2024-02-11 14:09:34,714 [INFO] loss: 0.0077    [33,    20]\n","2024-02-11 14:09:34,734 [INFO] loss: 0.0063    [34,    10]\n","2024-02-11 14:09:34,754 [INFO] loss: 0.0085    [34,    20]\n","2024-02-11 14:09:34,774 [INFO] loss: 0.0054    [35,    10]\n","2024-02-11 14:09:34,791 [INFO] loss: 0.0085    [35,    20]\n","2024-02-11 14:09:34,810 [INFO] loss: 0.0069    [36,    10]\n","2024-02-11 14:09:34,828 [INFO] loss: 0.0069    [36,    20]\n","2024-02-11 14:09:34,847 [INFO] loss: 0.0068    [37,    10]\n","2024-02-11 14:09:34,866 [INFO] loss: 0.0073    [37,    20]\n","2024-02-11 14:09:34,886 [INFO] loss: 0.0068    [38,    10]\n","2024-02-11 14:09:34,905 [INFO] loss: 0.0077    [38,    20]\n","2024-02-11 14:09:34,930 [INFO] loss: 0.0060    [39,    10]\n","2024-02-11 14:09:34,948 [INFO] loss: 0.0077    [39,    20]\n","2024-02-11 14:09:34,966 [INFO] loss: 0.0054    [40,    10]\n","2024-02-11 14:09:34,985 [INFO] loss: 0.0060    [40,    20]\n","2024-02-11 14:09:35,003 [INFO] loss: 0.0055    [41,    10]\n","2024-02-11 14:09:35,021 [INFO] loss: 0.0059    [41,    20]\n","2024-02-11 14:09:35,039 [INFO] loss: 0.0049    [42,    10]\n","2024-02-11 14:09:35,059 [INFO] loss: 0.0066    [42,    20]\n","2024-02-11 14:09:35,078 [INFO] loss: 0.0056    [43,    10]\n","2024-02-11 14:09:35,097 [INFO] loss: 0.0060    [43,    20]\n","2024-02-11 14:09:35,116 [INFO] loss: 0.0044    [44,    10]\n","2024-02-11 14:09:35,138 [INFO] loss: 0.0069    [44,    20]\n","2024-02-11 14:09:35,156 [INFO] loss: 0.0061    [45,    10]\n","2024-02-11 14:09:35,175 [INFO] loss: 0.0060    [45,    20]\n","2024-02-11 14:09:35,194 [INFO] loss: 0.0059    [46,    10]\n","2024-02-11 14:09:35,214 [INFO] loss: 0.0064    [46,    20]\n","2024-02-11 14:09:35,233 [INFO] loss: 0.0057    [47,    10]\n","2024-02-11 14:09:35,254 [INFO] loss: 0.0068    [47,    20]\n","2024-02-11 14:09:35,273 [INFO] loss: 0.0047    [48,    10]\n","2024-02-11 14:09:35,291 [INFO] loss: 0.0058    [48,    20]\n","2024-02-11 14:09:35,322 [INFO] loss: 0.0051    [49,    10]\n","2024-02-11 14:09:35,343 [INFO] loss: 0.0059    [49,    20]\n","2024-02-11 14:09:35,362 [INFO] loss: 0.0061    [50,    10]\n","2024-02-11 14:09:35,383 [INFO] loss: 0.0057    [50,    20]\n","2024-02-11 14:09:35,425 [INFO] Result on Train Data : {'AUC': 0.9911221590909091, 'ACC': 0.9490445859872612, 'F1 Score': 0.9490316624565034, 'AUPR': 0, 'Loss': 0.1379700256511569}\n"]}],"id":"OqKrF7HmNFx1"},{"cell_type":"markdown","source":["### Test"],"metadata":{"id":"0eQGNWm_NMVG"},"id":"0eQGNWm_NMVG"},{"cell_type":"code","source":["test_result = SimpleTester().test(model=mda_classifier,\n","                                  data=test_data,\n","                                  config=classifier_optimizer_config)"],"metadata":{"id":"U05mXL_fNHpG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660575610,"user_tz":-210,"elapsed":8,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"32afc0d3-211e-4ad0-b08d-edd4ab890b72"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 14:09:35,437 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:35,440 [INFO] moving data and model to cpu\n","2024-02-11 14:09:35,468 [INFO] Result on Test Data : {'AUC': 0.9743251415690802, 'ACC': 0.8962962962962963, 'F1 Score': 0.8962450592885376, 'AUPR': 0, 'Loss': 0.24296143651008606}\n"]}],"id":"U05mXL_fNHpG"},{"cell_type":"code","source":["test_result.get_result()"],"metadata":{"id":"oqgiZQqRWWGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660575982,"user_tz":-210,"elapsed":375,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"7739f936-28d5-443e-e57e-a6670c6823d1"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AUC': 0.9743251415690802,\n"," 'ACC': 0.8962962962962963,\n"," 'F1 Score': 0.8962450592885376,\n"," 'AUPR': 0,\n"," 'Loss': 0.24296143651008606}"]},"metadata":{},"execution_count":19}],"id":"oqgiZQqRWWGF"},{"cell_type":"markdown","source":["## Cross Validation"],"metadata":{"id":"ti8vEX_cNNwy"},"id":"ti8vEX_cNNwy"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707660588402,"user_tz":-210,"elapsed":12428,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}},"outputId":"f33c23f4-e62e-4ebf-ae40-9f6cfadbcc81","id":"sfI286uv6o-e"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-11 14:09:35,492 [INFO] Initializing SimpleMDAClassifierFactory with model : simple classifier\n","2024-02-11 14:09:35,493 [INFO] Initializing SimplePytorchDataTrainTestSplit\n","2024-02-11 14:09:35,495 [INFO] Start 5-fold Cross Validation with config : adam optimizer\n","2024-02-11 14:09:35,497 [INFO] ---- Fold 1 ----\n","2024-02-11 14:09:35,500 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 14:09:35,501 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 14:09:35,502 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:35,504 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 14:09:35,506 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:35,507 [INFO] moving data and model to cpu\n","2024-02-11 14:09:35,545 [INFO] loss: 0.0210    [1,    10]\n","2024-02-11 14:09:35,563 [INFO] loss: 0.0179    [1,    20]\n","2024-02-11 14:09:35,585 [INFO] loss: 0.0157    [2,    10]\n","2024-02-11 14:09:35,602 [INFO] loss: 0.0162    [2,    20]\n","2024-02-11 14:09:35,626 [INFO] loss: 0.0137    [3,    10]\n","2024-02-11 14:09:35,645 [INFO] loss: 0.0144    [3,    20]\n","2024-02-11 14:09:35,671 [INFO] loss: 0.0128    [4,    10]\n","2024-02-11 14:09:35,687 [INFO] loss: 0.0127    [4,    20]\n","2024-02-11 14:09:35,713 [INFO] loss: 0.0123    [5,    10]\n","2024-02-11 14:09:35,732 [INFO] loss: 0.0130    [5,    20]\n","2024-02-11 14:09:35,763 [INFO] loss: 0.0122    [6,    10]\n","2024-02-11 14:09:35,787 [INFO] loss: 0.0120    [6,    20]\n","2024-02-11 14:09:35,811 [INFO] loss: 0.0109    [7,    10]\n","2024-02-11 14:09:35,832 [INFO] loss: 0.0114    [7,    20]\n","2024-02-11 14:09:35,858 [INFO] loss: 0.0111    [8,    10]\n","2024-02-11 14:09:35,880 [INFO] loss: 0.0122    [8,    20]\n","2024-02-11 14:09:35,907 [INFO] loss: 0.0117    [9,    10]\n","2024-02-11 14:09:35,930 [INFO] loss: 0.0125    [9,    20]\n","2024-02-11 14:09:35,952 [INFO] loss: 0.0114    [10,    10]\n","2024-02-11 14:09:35,971 [INFO] loss: 0.0099    [10,    20]\n","2024-02-11 14:09:35,994 [INFO] loss: 0.0100    [11,    10]\n","2024-02-11 14:09:36,012 [INFO] loss: 0.0109    [11,    20]\n","2024-02-11 14:09:36,035 [INFO] loss: 0.0100    [12,    10]\n","2024-02-11 14:09:36,053 [INFO] loss: 0.0098    [12,    20]\n","2024-02-11 14:09:36,075 [INFO] loss: 0.0103    [13,    10]\n","2024-02-11 14:09:36,093 [INFO] loss: 0.0102    [13,    20]\n","2024-02-11 14:09:36,115 [INFO] loss: 0.0101    [14,    10]\n","2024-02-11 14:09:36,134 [INFO] loss: 0.0098    [14,    20]\n","2024-02-11 14:09:36,161 [INFO] loss: 0.0090    [15,    10]\n","2024-02-11 14:09:36,182 [INFO] loss: 0.0098    [15,    20]\n","2024-02-11 14:09:36,203 [INFO] loss: 0.0089    [16,    10]\n","2024-02-11 14:09:36,222 [INFO] loss: 0.0098    [16,    20]\n","2024-02-11 14:09:36,245 [INFO] loss: 0.0084    [17,    10]\n","2024-02-11 14:09:36,267 [INFO] loss: 0.0098    [17,    20]\n","2024-02-11 14:09:36,288 [INFO] loss: 0.0085    [18,    10]\n","2024-02-11 14:09:36,309 [INFO] loss: 0.0098    [18,    20]\n","2024-02-11 14:09:36,331 [INFO] loss: 0.0086    [19,    10]\n","2024-02-11 14:09:36,364 [INFO] loss: 0.0078    [19,    20]\n","2024-02-11 14:09:36,388 [INFO] loss: 0.0067    [20,    10]\n","2024-02-11 14:09:36,406 [INFO] loss: 0.0093    [20,    20]\n","2024-02-11 14:09:36,430 [INFO] loss: 0.0081    [21,    10]\n","2024-02-11 14:09:36,450 [INFO] loss: 0.0081    [21,    20]\n","2024-02-11 14:09:36,473 [INFO] loss: 0.0065    [22,    10]\n","2024-02-11 14:09:36,492 [INFO] loss: 0.0086    [22,    20]\n","2024-02-11 14:09:36,514 [INFO] loss: 0.0072    [23,    10]\n","2024-02-11 14:09:36,536 [INFO] loss: 0.0081    [23,    20]\n","2024-02-11 14:09:36,558 [INFO] loss: 0.0081    [24,    10]\n","2024-02-11 14:09:36,575 [INFO] loss: 0.0091    [24,    20]\n","2024-02-11 14:09:36,599 [INFO] loss: 0.0092    [25,    10]\n","2024-02-11 14:09:36,617 [INFO] loss: 0.0080    [25,    20]\n","2024-02-11 14:09:36,640 [INFO] loss: 0.0069    [26,    10]\n","2024-02-11 14:09:36,658 [INFO] loss: 0.0077    [26,    20]\n","2024-02-11 14:09:36,681 [INFO] loss: 0.0067    [27,    10]\n","2024-02-11 14:09:36,699 [INFO] loss: 0.0080    [27,    20]\n","2024-02-11 14:09:36,722 [INFO] loss: 0.0062    [28,    10]\n","2024-02-11 14:09:36,740 [INFO] loss: 0.0085    [28,    20]\n","2024-02-11 14:09:36,765 [INFO] loss: 0.0082    [29,    10]\n","2024-02-11 14:09:36,784 [INFO] loss: 0.0079    [29,    20]\n","2024-02-11 14:09:36,806 [INFO] loss: 0.0069    [30,    10]\n","2024-02-11 14:09:36,824 [INFO] loss: 0.0073    [30,    20]\n","2024-02-11 14:09:36,847 [INFO] loss: 0.0069    [31,    10]\n","2024-02-11 14:09:36,869 [INFO] loss: 0.0075    [31,    20]\n","2024-02-11 14:09:36,891 [INFO] loss: 0.0059    [32,    10]\n","2024-02-11 14:09:36,912 [INFO] loss: 0.0066    [32,    20]\n","2024-02-11 14:09:36,934 [INFO] loss: 0.0067    [33,    10]\n","2024-02-11 14:09:36,952 [INFO] loss: 0.0073    [33,    20]\n","2024-02-11 14:09:36,974 [INFO] loss: 0.0073    [34,    10]\n","2024-02-11 14:09:36,992 [INFO] loss: 0.0057    [34,    20]\n","2024-02-11 14:09:37,014 [INFO] loss: 0.0061    [35,    10]\n","2024-02-11 14:09:37,036 [INFO] loss: 0.0062    [35,    20]\n","2024-02-11 14:09:37,072 [INFO] loss: 0.0081    [36,    10]\n","2024-02-11 14:09:37,098 [INFO] loss: 0.0049    [36,    20]\n","2024-02-11 14:09:37,130 [INFO] loss: 0.0058    [37,    10]\n","2024-02-11 14:09:37,160 [INFO] loss: 0.0064    [37,    20]\n","2024-02-11 14:09:37,192 [INFO] loss: 0.0051    [38,    10]\n","2024-02-11 14:09:37,217 [INFO] loss: 0.0072    [38,    20]\n","2024-02-11 14:09:37,247 [INFO] loss: 0.0054    [39,    10]\n","2024-02-11 14:09:37,273 [INFO] loss: 0.0059    [39,    20]\n","2024-02-11 14:09:37,304 [INFO] loss: 0.0041    [40,    10]\n","2024-02-11 14:09:37,326 [INFO] loss: 0.0062    [40,    20]\n","2024-02-11 14:09:37,357 [INFO] loss: 0.0059    [41,    10]\n","2024-02-11 14:09:37,387 [INFO] loss: 0.0054    [41,    20]\n","2024-02-11 14:09:37,430 [INFO] loss: 0.0047    [42,    10]\n","2024-02-11 14:09:37,457 [INFO] loss: 0.0060    [42,    20]\n","2024-02-11 14:09:37,488 [INFO] loss: 0.0059    [43,    10]\n","2024-02-11 14:09:37,520 [INFO] loss: 0.0070    [43,    20]\n","2024-02-11 14:09:37,561 [INFO] loss: 0.0072    [44,    10]\n","2024-02-11 14:09:37,584 [INFO] loss: 0.0058    [44,    20]\n","2024-02-11 14:09:37,614 [INFO] loss: 0.0050    [45,    10]\n","2024-02-11 14:09:37,640 [INFO] loss: 0.0063    [45,    20]\n","2024-02-11 14:09:37,671 [INFO] loss: 0.0057    [46,    10]\n","2024-02-11 14:09:37,693 [INFO] loss: 0.0064    [46,    20]\n","2024-02-11 14:09:37,724 [INFO] loss: 0.0050    [47,    10]\n","2024-02-11 14:09:37,750 [INFO] loss: 0.0051    [47,    20]\n","2024-02-11 14:09:37,782 [INFO] loss: 0.0054    [48,    10]\n","2024-02-11 14:09:37,810 [INFO] loss: 0.0066    [48,    20]\n","2024-02-11 14:09:37,845 [INFO] loss: 0.0063    [49,    10]\n","2024-02-11 14:09:37,869 [INFO] loss: 0.0057    [49,    20]\n","2024-02-11 14:09:37,912 [INFO] loss: 0.0050    [50,    10]\n","2024-02-11 14:09:37,944 [INFO] loss: 0.0052    [50,    20]\n","2024-02-11 14:09:38,024 [INFO] Result on Train Data : {'AUC': 0.9929665738161559, 'ACC': 0.9568845618915159, 'F1 Score': 0.9568792235203532, 'AUPR': 0, 'Loss': 0.12208852690199147}\n","2024-02-11 14:09:38,030 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:38,032 [INFO] moving data and model to cpu\n","2024-02-11 14:09:38,060 [INFO] Result on Test Data : {'AUC': 0.870474406991261, 'ACC': 0.7932960893854749, 'F1 Score': 0.7923629181427722, 'AUPR': 0, 'Loss': 0.768635074297587}\n","2024-02-11 14:09:38,062 [INFO] Result of fold 1 : {'AUC': 0.870474406991261, 'ACC': 0.7932960893854749, 'F1 Score': 0.7923629181427722, 'AUPR': 0, 'Loss': 0.768635074297587}\n","2024-02-11 14:09:38,065 [INFO] ---- Fold 2 ----\n","2024-02-11 14:09:38,068 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 14:09:38,071 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 14:09:38,073 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:38,074 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 14:09:38,076 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:38,077 [INFO] moving data and model to cpu\n","2024-02-11 14:09:38,108 [INFO] loss: 0.0216    [1,    10]\n","2024-02-11 14:09:38,131 [INFO] loss: 0.0203    [1,    20]\n","2024-02-11 14:09:38,166 [INFO] loss: 0.0156    [2,    10]\n","2024-02-11 14:09:38,190 [INFO] loss: 0.0176    [2,    20]\n","2024-02-11 14:09:38,220 [INFO] loss: 0.0142    [3,    10]\n","2024-02-11 14:09:38,245 [INFO] loss: 0.0151    [3,    20]\n","2024-02-11 14:09:38,275 [INFO] loss: 0.0145    [4,    10]\n","2024-02-11 14:09:38,298 [INFO] loss: 0.0130    [4,    20]\n","2024-02-11 14:09:38,325 [INFO] loss: 0.0125    [5,    10]\n","2024-02-11 14:09:38,350 [INFO] loss: 0.0148    [5,    20]\n","2024-02-11 14:09:38,379 [INFO] loss: 0.0120    [6,    10]\n","2024-02-11 14:09:38,405 [INFO] loss: 0.0140    [6,    20]\n","2024-02-11 14:09:38,439 [INFO] loss: 0.0130    [7,    10]\n","2024-02-11 14:09:38,473 [INFO] loss: 0.0130    [7,    20]\n","2024-02-11 14:09:38,504 [INFO] loss: 0.0123    [8,    10]\n","2024-02-11 14:09:38,531 [INFO] loss: 0.0123    [8,    20]\n","2024-02-11 14:09:38,564 [INFO] loss: 0.0108    [9,    10]\n","2024-02-11 14:09:38,590 [INFO] loss: 0.0128    [9,    20]\n","2024-02-11 14:09:38,625 [INFO] loss: 0.0113    [10,    10]\n","2024-02-11 14:09:38,654 [INFO] loss: 0.0113    [10,    20]\n","2024-02-11 14:09:38,684 [INFO] loss: 0.0104    [11,    10]\n","2024-02-11 14:09:38,715 [INFO] loss: 0.0112    [11,    20]\n","2024-02-11 14:09:38,751 [INFO] loss: 0.0101    [12,    10]\n","2024-02-11 14:09:38,780 [INFO] loss: 0.0119    [12,    20]\n","2024-02-11 14:09:38,813 [INFO] loss: 0.0100    [13,    10]\n","2024-02-11 14:09:38,837 [INFO] loss: 0.0114    [13,    20]\n","2024-02-11 14:09:38,871 [INFO] loss: 0.0085    [14,    10]\n","2024-02-11 14:09:38,898 [INFO] loss: 0.0120    [14,    20]\n","2024-02-11 14:09:38,933 [INFO] loss: 0.0097    [15,    10]\n","2024-02-11 14:09:38,957 [INFO] loss: 0.0107    [15,    20]\n","2024-02-11 14:09:38,988 [INFO] loss: 0.0095    [16,    10]\n","2024-02-11 14:09:39,011 [INFO] loss: 0.0099    [16,    20]\n","2024-02-11 14:09:39,040 [INFO] loss: 0.0093    [17,    10]\n","2024-02-11 14:09:39,063 [INFO] loss: 0.0101    [17,    20]\n","2024-02-11 14:09:39,092 [INFO] loss: 0.0095    [18,    10]\n","2024-02-11 14:09:39,115 [INFO] loss: 0.0082    [18,    20]\n","2024-02-11 14:09:39,147 [INFO] loss: 0.0076    [19,    10]\n","2024-02-11 14:09:39,174 [INFO] loss: 0.0091    [19,    20]\n","2024-02-11 14:09:39,202 [INFO] loss: 0.0084    [20,    10]\n","2024-02-11 14:09:39,224 [INFO] loss: 0.0090    [20,    20]\n","2024-02-11 14:09:39,252 [INFO] loss: 0.0074    [21,    10]\n","2024-02-11 14:09:39,274 [INFO] loss: 0.0091    [21,    20]\n","2024-02-11 14:09:39,302 [INFO] loss: 0.0080    [22,    10]\n","2024-02-11 14:09:39,328 [INFO] loss: 0.0086    [22,    20]\n","2024-02-11 14:09:39,359 [INFO] loss: 0.0096    [23,    10]\n","2024-02-11 14:09:39,381 [INFO] loss: 0.0090    [23,    20]\n","2024-02-11 14:09:39,410 [INFO] loss: 0.0082    [24,    10]\n","2024-02-11 14:09:39,434 [INFO] loss: 0.0094    [24,    20]\n","2024-02-11 14:09:39,464 [INFO] loss: 0.0077    [25,    10]\n","2024-02-11 14:09:39,499 [INFO] loss: 0.0096    [25,    20]\n","2024-02-11 14:09:39,537 [INFO] loss: 0.0079    [26,    10]\n","2024-02-11 14:09:39,559 [INFO] loss: 0.0083    [26,    20]\n","2024-02-11 14:09:39,586 [INFO] loss: 0.0073    [27,    10]\n","2024-02-11 14:09:39,608 [INFO] loss: 0.0075    [27,    20]\n","2024-02-11 14:09:39,636 [INFO] loss: 0.0077    [28,    10]\n","2024-02-11 14:09:39,658 [INFO] loss: 0.0068    [28,    20]\n","2024-02-11 14:09:39,685 [INFO] loss: 0.0080    [29,    10]\n","2024-02-11 14:09:39,707 [INFO] loss: 0.0080    [29,    20]\n","2024-02-11 14:09:39,734 [INFO] loss: 0.0067    [30,    10]\n","2024-02-11 14:09:39,756 [INFO] loss: 0.0073    [30,    20]\n","2024-02-11 14:09:39,785 [INFO] loss: 0.0062    [31,    10]\n","2024-02-11 14:09:39,810 [INFO] loss: 0.0087    [31,    20]\n","2024-02-11 14:09:39,842 [INFO] loss: 0.0065    [32,    10]\n","2024-02-11 14:09:39,867 [INFO] loss: 0.0084    [32,    20]\n","2024-02-11 14:09:39,896 [INFO] loss: 0.0070    [33,    10]\n","2024-02-11 14:09:39,921 [INFO] loss: 0.0084    [33,    20]\n","2024-02-11 14:09:39,950 [INFO] loss: 0.0063    [34,    10]\n","2024-02-11 14:09:39,974 [INFO] loss: 0.0071    [34,    20]\n","2024-02-11 14:09:40,004 [INFO] loss: 0.0054    [35,    10]\n","2024-02-11 14:09:40,027 [INFO] loss: 0.0077    [35,    20]\n","2024-02-11 14:09:40,057 [INFO] loss: 0.0067    [36,    10]\n","2024-02-11 14:09:40,081 [INFO] loss: 0.0095    [36,    20]\n","2024-02-11 14:09:40,113 [INFO] loss: 0.0063    [37,    10]\n","2024-02-11 14:09:40,142 [INFO] loss: 0.0086    [37,    20]\n","2024-02-11 14:09:40,171 [INFO] loss: 0.0068    [38,    10]\n","2024-02-11 14:09:40,193 [INFO] loss: 0.0073    [38,    20]\n","2024-02-11 14:09:40,230 [INFO] loss: 0.0059    [39,    10]\n","2024-02-11 14:09:40,258 [INFO] loss: 0.0071    [39,    20]\n","2024-02-11 14:09:40,296 [INFO] loss: 0.0060    [40,    10]\n","2024-02-11 14:09:40,324 [INFO] loss: 0.0079    [40,    20]\n","2024-02-11 14:09:40,361 [INFO] loss: 0.0062    [41,    10]\n","2024-02-11 14:09:40,388 [INFO] loss: 0.0065    [41,    20]\n","2024-02-11 14:09:40,415 [INFO] loss: 0.0061    [42,    10]\n","2024-02-11 14:09:40,438 [INFO] loss: 0.0059    [42,    20]\n","2024-02-11 14:09:40,468 [INFO] loss: 0.0053    [43,    10]\n","2024-02-11 14:09:40,495 [INFO] loss: 0.0065    [43,    20]\n","2024-02-11 14:09:40,530 [INFO] loss: 0.0067    [44,    10]\n","2024-02-11 14:09:40,571 [INFO] loss: 0.0071    [44,    20]\n","2024-02-11 14:09:40,612 [INFO] loss: 0.0075    [45,    10]\n","2024-02-11 14:09:40,635 [INFO] loss: 0.0067    [45,    20]\n","2024-02-11 14:09:40,670 [INFO] loss: 0.0062    [46,    10]\n","2024-02-11 14:09:40,693 [INFO] loss: 0.0067    [46,    20]\n","2024-02-11 14:09:40,724 [INFO] loss: 0.0068    [47,    10]\n","2024-02-11 14:09:40,750 [INFO] loss: 0.0056    [47,    20]\n","2024-02-11 14:09:40,782 [INFO] loss: 0.0055    [48,    10]\n","2024-02-11 14:09:40,804 [INFO] loss: 0.0066    [48,    20]\n","2024-02-11 14:09:40,843 [INFO] loss: 0.0062    [49,    10]\n","2024-02-11 14:09:40,871 [INFO] loss: 0.0064    [49,    20]\n","2024-02-11 14:09:40,902 [INFO] loss: 0.0053    [50,    10]\n","2024-02-11 14:09:40,927 [INFO] loss: 0.0075    [50,    20]\n","2024-02-11 14:09:41,001 [INFO] Result on Train Data : {'AUC': 0.9881162653775837, 'ACC': 0.9304589707927677, 'F1 Score': 0.9304200730831165, 'AUPR': 0, 'Loss': 0.1521607624447864}\n","2024-02-11 14:09:41,003 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:41,005 [INFO] moving data and model to cpu\n","2024-02-11 14:09:41,034 [INFO] Result on Test Data : {'AUC': 0.8711484593837534, 'ACC': 0.7988826815642458, 'F1 Score': 0.7960759493670886, 'AUPR': 0, 'Loss': 0.7081197053194046}\n","2024-02-11 14:09:41,037 [INFO] Result of fold 2 : {'AUC': 0.8711484593837534, 'ACC': 0.7988826815642458, 'F1 Score': 0.7960759493670886, 'AUPR': 0, 'Loss': 0.7081197053194046}\n","2024-02-11 14:09:41,039 [INFO] ---- Fold 3 ----\n","2024-02-11 14:09:41,045 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 14:09:41,050 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 14:09:41,052 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:41,056 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 14:09:41,058 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:41,062 [INFO] moving data and model to cpu\n","2024-02-11 14:09:41,104 [INFO] loss: 0.0200    [1,    10]\n","2024-02-11 14:09:41,133 [INFO] loss: 0.0181    [1,    20]\n","2024-02-11 14:09:41,169 [INFO] loss: 0.0149    [2,    10]\n","2024-02-11 14:09:41,203 [INFO] loss: 0.0160    [2,    20]\n","2024-02-11 14:09:41,241 [INFO] loss: 0.0140    [3,    10]\n","2024-02-11 14:09:41,261 [INFO] loss: 0.0153    [3,    20]\n","2024-02-11 14:09:41,285 [INFO] loss: 0.0131    [4,    10]\n","2024-02-11 14:09:41,304 [INFO] loss: 0.0127    [4,    20]\n","2024-02-11 14:09:41,329 [INFO] loss: 0.0126    [5,    10]\n","2024-02-11 14:09:41,351 [INFO] loss: 0.0129    [5,    20]\n","2024-02-11 14:09:41,375 [INFO] loss: 0.0123    [6,    10]\n","2024-02-11 14:09:41,395 [INFO] loss: 0.0127    [6,    20]\n","2024-02-11 14:09:41,419 [INFO] loss: 0.0109    [7,    10]\n","2024-02-11 14:09:41,440 [INFO] loss: 0.0129    [7,    20]\n","2024-02-11 14:09:41,465 [INFO] loss: 0.0110    [8,    10]\n","2024-02-11 14:09:41,488 [INFO] loss: 0.0124    [8,    20]\n","2024-02-11 14:09:41,510 [INFO] loss: 0.0092    [9,    10]\n","2024-02-11 14:09:41,527 [INFO] loss: 0.0114    [9,    20]\n","2024-02-11 14:09:41,549 [INFO] loss: 0.0119    [10,    10]\n","2024-02-11 14:09:41,567 [INFO] loss: 0.0103    [10,    20]\n","2024-02-11 14:09:41,595 [INFO] loss: 0.0117    [11,    10]\n","2024-02-11 14:09:41,618 [INFO] loss: 0.0089    [11,    20]\n","2024-02-11 14:09:41,654 [INFO] loss: 0.0099    [12,    10]\n","2024-02-11 14:09:41,672 [INFO] loss: 0.0102    [12,    20]\n","2024-02-11 14:09:41,695 [INFO] loss: 0.0094    [13,    10]\n","2024-02-11 14:09:41,718 [INFO] loss: 0.0094    [13,    20]\n","2024-02-11 14:09:41,749 [INFO] loss: 0.0089    [14,    10]\n","2024-02-11 14:09:41,768 [INFO] loss: 0.0103    [14,    20]\n","2024-02-11 14:09:41,791 [INFO] loss: 0.0086    [15,    10]\n","2024-02-11 14:09:41,810 [INFO] loss: 0.0089    [15,    20]\n","2024-02-11 14:09:41,832 [INFO] loss: 0.0077    [16,    10]\n","2024-02-11 14:09:41,850 [INFO] loss: 0.0097    [16,    20]\n","2024-02-11 14:09:41,875 [INFO] loss: 0.0085    [17,    10]\n","2024-02-11 14:09:41,893 [INFO] loss: 0.0101    [17,    20]\n","2024-02-11 14:09:41,915 [INFO] loss: 0.0077    [18,    10]\n","2024-02-11 14:09:41,934 [INFO] loss: 0.0085    [18,    20]\n","2024-02-11 14:09:41,957 [INFO] loss: 0.0080    [19,    10]\n","2024-02-11 14:09:41,974 [INFO] loss: 0.0090    [19,    20]\n","2024-02-11 14:09:41,996 [INFO] loss: 0.0082    [20,    10]\n","2024-02-11 14:09:42,014 [INFO] loss: 0.0075    [20,    20]\n","2024-02-11 14:09:42,040 [INFO] loss: 0.0075    [21,    10]\n","2024-02-11 14:09:42,059 [INFO] loss: 0.0092    [21,    20]\n","2024-02-11 14:09:42,084 [INFO] loss: 0.0074    [22,    10]\n","2024-02-11 14:09:42,108 [INFO] loss: 0.0092    [22,    20]\n","2024-02-11 14:09:42,130 [INFO] loss: 0.0073    [23,    10]\n","2024-02-11 14:09:42,148 [INFO] loss: 0.0079    [23,    20]\n","2024-02-11 14:09:42,170 [INFO] loss: 0.0065    [24,    10]\n","2024-02-11 14:09:42,188 [INFO] loss: 0.0089    [24,    20]\n","2024-02-11 14:09:42,211 [INFO] loss: 0.0074    [25,    10]\n","2024-02-11 14:09:42,230 [INFO] loss: 0.0078    [25,    20]\n","2024-02-11 14:09:42,253 [INFO] loss: 0.0066    [26,    10]\n","2024-02-11 14:09:42,271 [INFO] loss: 0.0087    [26,    20]\n","2024-02-11 14:09:42,294 [INFO] loss: 0.0087    [27,    10]\n","2024-02-11 14:09:42,314 [INFO] loss: 0.0075    [27,    20]\n","2024-02-11 14:09:42,336 [INFO] loss: 0.0064    [28,    10]\n","2024-02-11 14:09:42,354 [INFO] loss: 0.0083    [28,    20]\n","2024-02-11 14:09:42,377 [INFO] loss: 0.0068    [29,    10]\n","2024-02-11 14:09:42,396 [INFO] loss: 0.0083    [29,    20]\n","2024-02-11 14:09:42,419 [INFO] loss: 0.0067    [30,    10]\n","2024-02-11 14:09:42,439 [INFO] loss: 0.0071    [30,    20]\n","2024-02-11 14:09:42,462 [INFO] loss: 0.0066    [31,    10]\n","2024-02-11 14:09:42,482 [INFO] loss: 0.0077    [31,    20]\n","2024-02-11 14:09:42,507 [INFO] loss: 0.0060    [32,    10]\n","2024-02-11 14:09:42,526 [INFO] loss: 0.0071    [32,    20]\n","2024-02-11 14:09:42,549 [INFO] loss: 0.0074    [33,    10]\n","2024-02-11 14:09:42,568 [INFO] loss: 0.0066    [33,    20]\n","2024-02-11 14:09:42,592 [INFO] loss: 0.0069    [34,    10]\n","2024-02-11 14:09:42,611 [INFO] loss: 0.0071    [34,    20]\n","2024-02-11 14:09:42,638 [INFO] loss: 0.0071    [35,    10]\n","2024-02-11 14:09:42,658 [INFO] loss: 0.0071    [35,    20]\n","2024-02-11 14:09:42,693 [INFO] loss: 0.0070    [36,    10]\n","2024-02-11 14:09:42,711 [INFO] loss: 0.0051    [36,    20]\n","2024-02-11 14:09:42,735 [INFO] loss: 0.0056    [37,    10]\n","2024-02-11 14:09:42,755 [INFO] loss: 0.0063    [37,    20]\n","2024-02-11 14:09:42,780 [INFO] loss: 0.0054    [38,    10]\n","2024-02-11 14:09:42,798 [INFO] loss: 0.0062    [38,    20]\n","2024-02-11 14:09:42,823 [INFO] loss: 0.0051    [39,    10]\n","2024-02-11 14:09:42,841 [INFO] loss: 0.0070    [39,    20]\n","2024-02-11 14:09:42,864 [INFO] loss: 0.0061    [40,    10]\n","2024-02-11 14:09:42,884 [INFO] loss: 0.0072    [40,    20]\n","2024-02-11 14:09:42,905 [INFO] loss: 0.0064    [41,    10]\n","2024-02-11 14:09:42,924 [INFO] loss: 0.0068    [41,    20]\n","2024-02-11 14:09:42,946 [INFO] loss: 0.0077    [42,    10]\n","2024-02-11 14:09:42,964 [INFO] loss: 0.0066    [42,    20]\n","2024-02-11 14:09:42,986 [INFO] loss: 0.0050    [43,    10]\n","2024-02-11 14:09:43,005 [INFO] loss: 0.0066    [43,    20]\n","2024-02-11 14:09:43,027 [INFO] loss: 0.0065    [44,    10]\n","2024-02-11 14:09:43,046 [INFO] loss: 0.0061    [44,    20]\n","2024-02-11 14:09:43,069 [INFO] loss: 0.0056    [45,    10]\n","2024-02-11 14:09:43,089 [INFO] loss: 0.0050    [45,    20]\n","2024-02-11 14:09:43,112 [INFO] loss: 0.0070    [46,    10]\n","2024-02-11 14:09:43,131 [INFO] loss: 0.0061    [46,    20]\n","2024-02-11 14:09:43,155 [INFO] loss: 0.0052    [47,    10]\n","2024-02-11 14:09:43,173 [INFO] loss: 0.0069    [47,    20]\n","2024-02-11 14:09:43,196 [INFO] loss: 0.0053    [48,    10]\n","2024-02-11 14:09:43,215 [INFO] loss: 0.0069    [48,    20]\n","2024-02-11 14:09:43,240 [INFO] loss: 0.0047    [49,    10]\n","2024-02-11 14:09:43,259 [INFO] loss: 0.0076    [49,    20]\n","2024-02-11 14:09:43,284 [INFO] loss: 0.0058    [50,    10]\n","2024-02-11 14:09:43,302 [INFO] loss: 0.0054    [50,    20]\n","2024-02-11 14:09:43,349 [INFO] Result on Train Data : {'AUC': 0.9901904570038278, 'ACC': 0.9374130737134909, 'F1 Score': 0.9372888550346263, 'AUPR': 0, 'Loss': 0.13596234341030536}\n","2024-02-11 14:09:43,351 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:43,353 [INFO] moving data and model to cpu\n","2024-02-11 14:09:43,376 [INFO] Result on Test Data : {'AUC': 0.8673990802248339, 'ACC': 0.770949720670391, 'F1 Score': 0.7680541070130527, 'AUPR': 0, 'Loss': 0.6576553682486216}\n","2024-02-11 14:09:43,378 [INFO] Result of fold 3 : {'AUC': 0.8673990802248339, 'ACC': 0.770949720670391, 'F1 Score': 0.7680541070130527, 'AUPR': 0, 'Loss': 0.6576553682486216}\n","2024-02-11 14:09:43,380 [INFO] ---- Fold 4 ----\n","2024-02-11 14:09:43,384 [INFO] Initializing SimplePytorchData with X shape : torch.Size([719, 64]) and y shape : torch.Size([719, 1])\n","2024-02-11 14:09:43,386 [INFO] Initializing SimplePytorchData with X shape : torch.Size([179, 64]) and y shape : torch.Size([179, 1])\n","2024-02-11 14:09:43,388 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:43,389 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 14:09:43,392 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:43,393 [INFO] moving data and model to cpu\n","2024-02-11 14:09:43,422 [INFO] loss: 0.0196    [1,    10]\n","2024-02-11 14:09:43,441 [INFO] loss: 0.0178    [1,    20]\n","2024-02-11 14:09:43,467 [INFO] loss: 0.0146    [2,    10]\n","2024-02-11 14:09:43,488 [INFO] loss: 0.0142    [2,    20]\n","2024-02-11 14:09:43,510 [INFO] loss: 0.0140    [3,    10]\n","2024-02-11 14:09:43,531 [INFO] loss: 0.0140    [3,    20]\n","2024-02-11 14:09:43,556 [INFO] loss: 0.0133    [4,    10]\n","2024-02-11 14:09:43,575 [INFO] loss: 0.0135    [4,    20]\n","2024-02-11 14:09:43,600 [INFO] loss: 0.0123    [5,    10]\n","2024-02-11 14:09:43,620 [INFO] loss: 0.0131    [5,    20]\n","2024-02-11 14:09:43,644 [INFO] loss: 0.0112    [6,    10]\n","2024-02-11 14:09:43,663 [INFO] loss: 0.0137    [6,    20]\n","2024-02-11 14:09:43,688 [INFO] loss: 0.0107    [7,    10]\n","2024-02-11 14:09:43,715 [INFO] loss: 0.0129    [7,    20]\n","2024-02-11 14:09:43,741 [INFO] loss: 0.0113    [8,    10]\n","2024-02-11 14:09:43,759 [INFO] loss: 0.0120    [8,    20]\n","2024-02-11 14:09:43,785 [INFO] loss: 0.0101    [9,    10]\n","2024-02-11 14:09:43,804 [INFO] loss: 0.0118    [9,    20]\n","2024-02-11 14:09:43,826 [INFO] loss: 0.0097    [10,    10]\n","2024-02-11 14:09:43,851 [INFO] loss: 0.0117    [10,    20]\n","2024-02-11 14:09:43,875 [INFO] loss: 0.0117    [11,    10]\n","2024-02-11 14:09:43,897 [INFO] loss: 0.0110    [11,    20]\n","2024-02-11 14:09:43,920 [INFO] loss: 0.0088    [12,    10]\n","2024-02-11 14:09:43,940 [INFO] loss: 0.0109    [12,    20]\n","2024-02-11 14:09:43,963 [INFO] loss: 0.0095    [13,    10]\n","2024-02-11 14:09:43,981 [INFO] loss: 0.0108    [13,    20]\n","2024-02-11 14:09:44,004 [INFO] loss: 0.0094    [14,    10]\n","2024-02-11 14:09:44,021 [INFO] loss: 0.0093    [14,    20]\n","2024-02-11 14:09:44,043 [INFO] loss: 0.0090    [15,    10]\n","2024-02-11 14:09:44,062 [INFO] loss: 0.0100    [15,    20]\n","2024-02-11 14:09:44,085 [INFO] loss: 0.0090    [16,    10]\n","2024-02-11 14:09:44,103 [INFO] loss: 0.0084    [16,    20]\n","2024-02-11 14:09:44,126 [INFO] loss: 0.0079    [17,    10]\n","2024-02-11 14:09:44,145 [INFO] loss: 0.0087    [17,    20]\n","2024-02-11 14:09:44,168 [INFO] loss: 0.0085    [18,    10]\n","2024-02-11 14:09:44,190 [INFO] loss: 0.0082    [18,    20]\n","2024-02-11 14:09:44,212 [INFO] loss: 0.0088    [19,    10]\n","2024-02-11 14:09:44,230 [INFO] loss: 0.0088    [19,    20]\n","2024-02-11 14:09:44,254 [INFO] loss: 0.0083    [20,    10]\n","2024-02-11 14:09:44,273 [INFO] loss: 0.0092    [20,    20]\n","2024-02-11 14:09:44,299 [INFO] loss: 0.0074    [21,    10]\n","2024-02-11 14:09:44,318 [INFO] loss: 0.0090    [21,    20]\n","2024-02-11 14:09:44,341 [INFO] loss: 0.0074    [22,    10]\n","2024-02-11 14:09:44,359 [INFO] loss: 0.0081    [22,    20]\n","2024-02-11 14:09:44,384 [INFO] loss: 0.0084    [23,    10]\n","2024-02-11 14:09:44,403 [INFO] loss: 0.0075    [23,    20]\n","2024-02-11 14:09:44,425 [INFO] loss: 0.0074    [24,    10]\n","2024-02-11 14:09:44,448 [INFO] loss: 0.0088    [24,    20]\n","2024-02-11 14:09:44,471 [INFO] loss: 0.0076    [25,    10]\n","2024-02-11 14:09:44,493 [INFO] loss: 0.0071    [25,    20]\n","2024-02-11 14:09:44,516 [INFO] loss: 0.0068    [26,    10]\n","2024-02-11 14:09:44,534 [INFO] loss: 0.0087    [26,    20]\n","2024-02-11 14:09:44,557 [INFO] loss: 0.0079    [27,    10]\n","2024-02-11 14:09:44,575 [INFO] loss: 0.0071    [27,    20]\n","2024-02-11 14:09:44,601 [INFO] loss: 0.0068    [28,    10]\n","2024-02-11 14:09:44,624 [INFO] loss: 0.0092    [28,    20]\n","2024-02-11 14:09:44,652 [INFO] loss: 0.0067    [29,    10]\n","2024-02-11 14:09:44,671 [INFO] loss: 0.0082    [29,    20]\n","2024-02-11 14:09:44,693 [INFO] loss: 0.0074    [30,    10]\n","2024-02-11 14:09:44,712 [INFO] loss: 0.0079    [30,    20]\n","2024-02-11 14:09:44,740 [INFO] loss: 0.0077    [31,    10]\n","2024-02-11 14:09:44,769 [INFO] loss: 0.0072    [31,    20]\n","2024-02-11 14:09:44,791 [INFO] loss: 0.0075    [32,    10]\n","2024-02-11 14:09:44,809 [INFO] loss: 0.0055    [32,    20]\n","2024-02-11 14:09:44,834 [INFO] loss: 0.0059    [33,    10]\n","2024-02-11 14:09:44,853 [INFO] loss: 0.0080    [33,    20]\n","2024-02-11 14:09:44,878 [INFO] loss: 0.0065    [34,    10]\n","2024-02-11 14:09:44,899 [INFO] loss: 0.0070    [34,    20]\n","2024-02-11 14:09:44,921 [INFO] loss: 0.0064    [35,    10]\n","2024-02-11 14:09:44,942 [INFO] loss: 0.0064    [35,    20]\n","2024-02-11 14:09:44,965 [INFO] loss: 0.0060    [36,    10]\n","2024-02-11 14:09:44,983 [INFO] loss: 0.0063    [36,    20]\n","2024-02-11 14:09:45,005 [INFO] loss: 0.0058    [37,    10]\n","2024-02-11 14:09:45,022 [INFO] loss: 0.0073    [37,    20]\n","2024-02-11 14:09:45,045 [INFO] loss: 0.0060    [38,    10]\n","2024-02-11 14:09:45,064 [INFO] loss: 0.0068    [38,    20]\n","2024-02-11 14:09:45,088 [INFO] loss: 0.0054    [39,    10]\n","2024-02-11 14:09:45,107 [INFO] loss: 0.0063    [39,    20]\n","2024-02-11 14:09:45,132 [INFO] loss: 0.0060    [40,    10]\n","2024-02-11 14:09:45,153 [INFO] loss: 0.0060    [40,    20]\n","2024-02-11 14:09:45,177 [INFO] loss: 0.0053    [41,    10]\n","2024-02-11 14:09:45,196 [INFO] loss: 0.0071    [41,    20]\n","2024-02-11 14:09:45,220 [INFO] loss: 0.0058    [42,    10]\n","2024-02-11 14:09:45,240 [INFO] loss: 0.0067    [42,    20]\n","2024-02-11 14:09:45,262 [INFO] loss: 0.0050    [43,    10]\n","2024-02-11 14:09:45,281 [INFO] loss: 0.0073    [43,    20]\n","2024-02-11 14:09:45,305 [INFO] loss: 0.0052    [44,    10]\n","2024-02-11 14:09:45,323 [INFO] loss: 0.0061    [44,    20]\n","2024-02-11 14:09:45,349 [INFO] loss: 0.0053    [45,    10]\n","2024-02-11 14:09:45,373 [INFO] loss: 0.0066    [45,    20]\n","2024-02-11 14:09:45,398 [INFO] loss: 0.0066    [46,    10]\n","2024-02-11 14:09:45,416 [INFO] loss: 0.0050    [46,    20]\n","2024-02-11 14:09:45,439 [INFO] loss: 0.0049    [47,    10]\n","2024-02-11 14:09:45,458 [INFO] loss: 0.0067    [47,    20]\n","2024-02-11 14:09:45,482 [INFO] loss: 0.0053    [48,    10]\n","2024-02-11 14:09:45,501 [INFO] loss: 0.0063    [48,    20]\n","2024-02-11 14:09:45,524 [INFO] loss: 0.0054    [49,    10]\n","2024-02-11 14:09:45,542 [INFO] loss: 0.0061    [49,    20]\n","2024-02-11 14:09:45,564 [INFO] loss: 0.0062    [50,    10]\n","2024-02-11 14:09:45,583 [INFO] loss: 0.0066    [50,    20]\n","2024-02-11 14:09:45,628 [INFO] Result on Train Data : {'AUC': 0.9852112676056339, 'ACC': 0.9346314325452016, 'F1 Score': 0.934517431078884, 'AUPR': 0, 'Loss': 0.16267495327021764}\n","2024-02-11 14:09:45,630 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:45,633 [INFO] moving data and model to cpu\n","2024-02-11 14:09:45,665 [INFO] Result on Test Data : {'AUC': 0.8156445556946184, 'ACC': 0.7597765363128491, 'F1 Score': 0.7595063271363849, 'AUPR': 0, 'Loss': 0.958420475323995}\n","2024-02-11 14:09:45,667 [INFO] Result of fold 4 : {'AUC': 0.8156445556946184, 'ACC': 0.7597765363128491, 'F1 Score': 0.7595063271363849, 'AUPR': 0, 'Loss': 0.958420475323995}\n","2024-02-11 14:09:45,669 [INFO] ---- Fold 5 ----\n","2024-02-11 14:09:45,673 [INFO] Initializing SimplePytorchData with X shape : torch.Size([716, 64]) and y shape : torch.Size([716, 1])\n","2024-02-11 14:09:45,676 [INFO] Initializing SimplePytorchData with X shape : torch.Size([182, 64]) and y shape : torch.Size([182, 1])\n","2024-02-11 14:09:45,678 [INFO] Initializing SimpleMDAClassifier with model : simple classifier\n","2024-02-11 14:09:45,679 [INFO] Initial SimpleMLP with 64 input dimension, 32 hidden dimension, 1 \n","            output dimension, 2 layers and with 0.1 dropout\n","2024-02-11 14:09:45,681 [INFO] Running Simple Trainer with config : adam optimizer\n","2024-02-11 14:09:45,682 [INFO] moving data and model to cpu\n","2024-02-11 14:09:45,717 [INFO] loss: 0.0229    [1,    10]\n","2024-02-11 14:09:45,734 [INFO] loss: 0.0191    [1,    20]\n","2024-02-11 14:09:45,761 [INFO] loss: 0.0158    [2,    10]\n","2024-02-11 14:09:45,788 [INFO] loss: 0.0156    [2,    20]\n","2024-02-11 14:09:45,813 [INFO] loss: 0.0138    [3,    10]\n","2024-02-11 14:09:45,830 [INFO] loss: 0.0145    [3,    20]\n","2024-02-11 14:09:45,854 [INFO] loss: 0.0137    [4,    10]\n","2024-02-11 14:09:45,872 [INFO] loss: 0.0126    [4,    20]\n","2024-02-11 14:09:45,899 [INFO] loss: 0.0131    [5,    10]\n","2024-02-11 14:09:45,917 [INFO] loss: 0.0128    [5,    20]\n","2024-02-11 14:09:45,940 [INFO] loss: 0.0124    [6,    10]\n","2024-02-11 14:09:45,958 [INFO] loss: 0.0121    [6,    20]\n","2024-02-11 14:09:45,984 [INFO] loss: 0.0133    [7,    10]\n","2024-02-11 14:09:46,002 [INFO] loss: 0.0125    [7,    20]\n","2024-02-11 14:09:46,028 [INFO] loss: 0.0110    [8,    10]\n","2024-02-11 14:09:46,051 [INFO] loss: 0.0122    [8,    20]\n","2024-02-11 14:09:46,074 [INFO] loss: 0.0113    [9,    10]\n","2024-02-11 14:09:46,093 [INFO] loss: 0.0120    [9,    20]\n","2024-02-11 14:09:46,116 [INFO] loss: 0.0114    [10,    10]\n","2024-02-11 14:09:46,134 [INFO] loss: 0.0101    [10,    20]\n","2024-02-11 14:09:46,157 [INFO] loss: 0.0103    [11,    10]\n","2024-02-11 14:09:46,175 [INFO] loss: 0.0111    [11,    20]\n","2024-02-11 14:09:46,197 [INFO] loss: 0.0105    [12,    10]\n","2024-02-11 14:09:46,216 [INFO] loss: 0.0110    [12,    20]\n","2024-02-11 14:09:46,239 [INFO] loss: 0.0110    [13,    10]\n","2024-02-11 14:09:46,258 [INFO] loss: 0.0097    [13,    20]\n","2024-02-11 14:09:46,281 [INFO] loss: 0.0086    [14,    10]\n","2024-02-11 14:09:46,300 [INFO] loss: 0.0095    [14,    20]\n","2024-02-11 14:09:46,322 [INFO] loss: 0.0091    [15,    10]\n","2024-02-11 14:09:46,342 [INFO] loss: 0.0101    [15,    20]\n","2024-02-11 14:09:46,365 [INFO] loss: 0.0077    [16,    10]\n","2024-02-11 14:09:46,386 [INFO] loss: 0.0100    [16,    20]\n","2024-02-11 14:09:46,408 [INFO] loss: 0.0080    [17,    10]\n","2024-02-11 14:09:46,428 [INFO] loss: 0.0096    [17,    20]\n","2024-02-11 14:09:46,451 [INFO] loss: 0.0094    [18,    10]\n","2024-02-11 14:09:46,470 [INFO] loss: 0.0088    [18,    20]\n","2024-02-11 14:09:46,501 [INFO] loss: 0.0089    [19,    10]\n","2024-02-11 14:09:46,520 [INFO] loss: 0.0087    [19,    20]\n","2024-02-11 14:09:46,542 [INFO] loss: 0.0085    [20,    10]\n","2024-02-11 14:09:46,563 [INFO] loss: 0.0077    [20,    20]\n","2024-02-11 14:09:46,588 [INFO] loss: 0.0081    [21,    10]\n","2024-02-11 14:09:46,606 [INFO] loss: 0.0080    [21,    20]\n","2024-02-11 14:09:46,630 [INFO] loss: 0.0084    [22,    10]\n","2024-02-11 14:09:46,649 [INFO] loss: 0.0081    [22,    20]\n","2024-02-11 14:09:46,672 [INFO] loss: 0.0080    [23,    10]\n","2024-02-11 14:09:46,692 [INFO] loss: 0.0080    [23,    20]\n","2024-02-11 14:09:46,715 [INFO] loss: 0.0066    [24,    10]\n","2024-02-11 14:09:46,733 [INFO] loss: 0.0085    [24,    20]\n","2024-02-11 14:09:46,757 [INFO] loss: 0.0074    [25,    10]\n","2024-02-11 14:09:46,776 [INFO] loss: 0.0089    [25,    20]\n","2024-02-11 14:09:46,800 [INFO] loss: 0.0079    [26,    10]\n","2024-02-11 14:09:46,828 [INFO] loss: 0.0091    [26,    20]\n","2024-02-11 14:09:46,854 [INFO] loss: 0.0072    [27,    10]\n","2024-02-11 14:09:46,874 [INFO] loss: 0.0076    [27,    20]\n","2024-02-11 14:09:46,901 [INFO] loss: 0.0074    [28,    10]\n","2024-02-11 14:09:46,922 [INFO] loss: 0.0085    [28,    20]\n","2024-02-11 14:09:46,946 [INFO] loss: 0.0075    [29,    10]\n","2024-02-11 14:09:46,965 [INFO] loss: 0.0083    [29,    20]\n","2024-02-11 14:09:46,988 [INFO] loss: 0.0076    [30,    10]\n","2024-02-11 14:09:47,007 [INFO] loss: 0.0080    [30,    20]\n","2024-02-11 14:09:47,030 [INFO] loss: 0.0070    [31,    10]\n","2024-02-11 14:09:47,048 [INFO] loss: 0.0078    [31,    20]\n","2024-02-11 14:09:47,078 [INFO] loss: 0.0064    [32,    10]\n","2024-02-11 14:09:47,103 [INFO] loss: 0.0073    [32,    20]\n","2024-02-11 14:09:47,131 [INFO] loss: 0.0072    [33,    10]\n","2024-02-11 14:09:47,155 [INFO] loss: 0.0075    [33,    20]\n","2024-02-11 14:09:47,184 [INFO] loss: 0.0061    [34,    10]\n","2024-02-11 14:09:47,209 [INFO] loss: 0.0068    [34,    20]\n","2024-02-11 14:09:47,239 [INFO] loss: 0.0061    [35,    10]\n","2024-02-11 14:09:47,264 [INFO] loss: 0.0067    [35,    20]\n","2024-02-11 14:09:47,290 [INFO] loss: 0.0071    [36,    10]\n","2024-02-11 14:09:47,309 [INFO] loss: 0.0066    [36,    20]\n","2024-02-11 14:09:47,332 [INFO] loss: 0.0083    [37,    10]\n","2024-02-11 14:09:47,350 [INFO] loss: 0.0055    [37,    20]\n","2024-02-11 14:09:47,375 [INFO] loss: 0.0072    [38,    10]\n","2024-02-11 14:09:47,393 [INFO] loss: 0.0063    [38,    20]\n","2024-02-11 14:09:47,417 [INFO] loss: 0.0061    [39,    10]\n","2024-02-11 14:09:47,436 [INFO] loss: 0.0066    [39,    20]\n","2024-02-11 14:09:47,463 [INFO] loss: 0.0065    [40,    10]\n","2024-02-11 14:09:47,485 [INFO] loss: 0.0074    [40,    20]\n","2024-02-11 14:09:47,510 [INFO] loss: 0.0060    [41,    10]\n","2024-02-11 14:09:47,530 [INFO] loss: 0.0077    [41,    20]\n","2024-02-11 14:09:47,553 [INFO] loss: 0.0070    [42,    10]\n","2024-02-11 14:09:47,572 [INFO] loss: 0.0057    [42,    20]\n","2024-02-11 14:09:47,595 [INFO] loss: 0.0047    [43,    10]\n","2024-02-11 14:09:47,615 [INFO] loss: 0.0075    [43,    20]\n","2024-02-11 14:09:47,637 [INFO] loss: 0.0066    [44,    10]\n","2024-02-11 14:09:47,656 [INFO] loss: 0.0063    [44,    20]\n","2024-02-11 14:09:47,680 [INFO] loss: 0.0064    [45,    10]\n","2024-02-11 14:09:47,698 [INFO] loss: 0.0060    [45,    20]\n","2024-02-11 14:09:47,721 [INFO] loss: 0.0064    [46,    10]\n","2024-02-11 14:09:47,739 [INFO] loss: 0.0059    [46,    20]\n","2024-02-11 14:09:47,762 [INFO] loss: 0.0059    [47,    10]\n","2024-02-11 14:09:47,781 [INFO] loss: 0.0068    [47,    20]\n","2024-02-11 14:09:47,804 [INFO] loss: 0.0063    [48,    10]\n","2024-02-11 14:09:47,824 [INFO] loss: 0.0054    [48,    20]\n","2024-02-11 14:09:47,854 [INFO] loss: 0.0055    [49,    10]\n","2024-02-11 14:09:47,881 [INFO] loss: 0.0052    [49,    20]\n","2024-02-11 14:09:47,906 [INFO] loss: 0.0074    [50,    10]\n","2024-02-11 14:09:47,924 [INFO] loss: 0.0060    [50,    20]\n","2024-02-11 14:09:47,970 [INFO] Result on Train Data : {'AUC': 0.9867710414585416, 'ACC': 0.9357541899441341, 'F1 Score': 0.9356693620844565, 'AUPR': 0, 'Loss': 0.16916501781214838}\n","2024-02-11 14:09:47,972 [INFO] Running Simple Tester with config : adam optimizer\n","2024-02-11 14:09:47,974 [INFO] moving data and model to cpu\n","2024-02-11 14:09:48,001 [INFO] Result on Test Data : {'AUC': 0.8488781079442086, 'ACC': 0.7637362637362637, 'F1 Score': 0.7636720519402083, 'AUPR': 0, 'Loss': 0.8208662867546082}\n","2024-02-11 14:09:48,003 [INFO] Result of fold 5 : {'AUC': 0.8488781079442086, 'ACC': 0.7637362637362637, 'F1 Score': 0.7636720519402083, 'AUPR': 0, 'Loss': 0.8208662867546082}\n","2024-02-11 14:09:48,006 [INFO] 5-fold result: avg_auc: 0.854708922047735, avg_acc: 0.7773282583338449, avg_f1: 0.7759342707199013, avg_aupr: 0.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<base.evaluation.Result at 0x7d18d7c4b850>"]},"metadata":{},"execution_count":20}],"source":["trainer = SimpleTrainer()\n","tester = SimpleTester()\n","factory = SimpleMDAClassifierFactory(simple_classifier_config)\n","spliter = SimplePytorchDataTrainTestSplit(data)\n","cross_validation(k=5, data_size=data.X.shape[0], train_test_spliter=spliter, model_factory=factory,\n","                    trainer=trainer, tester=tester, config=classifier_optimizer_config)"],"id":"sfI286uv6o-e"},{"cell_type":"code","source":[],"metadata":{"id":"5cpQ0kPkjuHb","executionInfo":{"status":"ok","timestamp":1707660588402,"user_tz":-210,"elapsed":18,"user":{"displayName":"Sobhan Ahmadian Moghadam","userId":"12456655244096551013"}}},"execution_count":20,"outputs":[],"id":"5cpQ0kPkjuHb"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}